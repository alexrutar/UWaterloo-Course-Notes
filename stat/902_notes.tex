
% header -----------------------------------------------------------------------
% Template created by texnew (author: Alex Rutar); info can be found at 'https://github.com/alexrutar/texnew'.
% version (1.13)


% doctype ----------------------------------------------------------------------
\documentclass[11pt, a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
\usepackage[protrusion=true,expansion=true]{microtype}


% packages ---------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{braket}

% Set enimitem
\usepackage{enumitem}
\SetEnumitemKey{nl}{nolistsep}
\SetEnumitemKey{r}{label=(\roman*)}

% Set tikz
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.15}
\usetikzlibrary{intersections,positioning,cd}
\usetikzlibrary{arrows,arrows.meta}
\tikzcdset{arrow style=tikz,diagrams={>=stealth}}

% Set hyperref
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{orange!50!yellow}
\colorlet{myurlcolor}{green!50!blue}

\hypersetup{
  linkcolor  = mylinkcolor!\myshade!black,
  citecolor  = mycitecolor!\myshade!black,
  urlcolor   = myurlcolor!\myshade!black,
  colorlinks = true,
}


% macros -----------------------------------------------------------------------
\DeclareMathOperator{\N}{{\mathbb{N}}}
\DeclareMathOperator{\Q}{{\mathbb{Q}}}
\DeclareMathOperator{\Z}{{\mathbb{Z}}}
\DeclareMathOperator{\R}{{\mathbb{R}}}
\DeclareMathOperator{\C}{{\mathbb{C}}}
\DeclareMathOperator{\F}{{\mathbb{F}}}

% Boldface includes math
\newcommand{\mbf}[1]{{\boldmath\bfseries #1}}

% proof implications
\newcommand{\imp}[2]{($#1\Rightarrow#2$)\hspace{0.2cm}}
\newcommand{\impe}[2]{($#1\Leftrightarrow#2$)\hspace{0.2cm}}
\newcommand{\impr}{{($\Longrightarrow$)\hspace{0.2cm}}}
\newcommand{\impl}{{($\Longleftarrow$)\hspace{0.2cm}}}

% align macros
\newcommand{\agspace}{\ensuremath{\phantom{--}}}
\newcommand{\agvdots}{\ensuremath{\hspace{0.16cm}\vdots}}

% convenient brackets
\newcommand{\brac}[1]{\ensuremath{\left\langle #1 \right\rangle}}
\newcommand{\norm}[1]{\ensuremath{\left\lVert#1\right\rVert}}
\newcommand{\abs}[1]{\ensuremath{\left\lvert#1\right\rvert}}

% arrows
\newcommand{\lto}[0]{\ensuremath{\longrightarrow}}
\newcommand{\fto}[1]{\ensuremath{\xrightarrow{\scriptstyle{#1}}}}
\newcommand{\hto}[0]{\ensuremath{\hookrightarrow}}
\newcommand{\mapsfrom}[0]{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}
 
% Divides, Not Divides
\renewcommand{\div}{\bigm|}
\newcommand{\ndiv}{%
    \mathrel{\mkern.5mu % small adjustment
        % superimpose \nmid to \big|
        \ooalign{\hidewidth$\big|$\hidewidth\cr$/$\cr}%
    }%
}

% Convenient overline
\newcommand{\ol}[1]{\ensuremath{\overline{#1}}}

% Big \cdot
\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

% Big and small Disjoint union
\makeatletter
\providecommand*{\cupdot}{%
  \mathbin{%
    \mathpalette\@cupdot{}%
  }%
}
\newcommand*{\@cupdot}[2]{%
  \ooalign{%
    $\m@th#1\cup$\cr
    \sbox0{$#1\cup$}%
    \dimen@=\ht0 %
    \sbox0{$\m@th#1\cdot$}%
    \advance\dimen@ by -\ht0 %
    \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }%
}

\providecommand*{\bigcupdot}{%
  \mathop{%
    \vphantom{\bigcup}%
    \mathpalette\@bigcupdot{}%
  }%
}
\newcommand*{\@bigcupdot}[2]{%
  \ooalign{%
    $\m@th#1\bigcup$\cr
    \sbox0{$#1\bigcup$}%
    \dimen@=\ht0 %
    \advance\dimen@ by -\dp0 %
    \sbox0{\scalebox{2}{$\m@th#1\cdot$}}%
    \advance\dimen@ by -\ht0 %
    \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }%
}
\makeatother


% macros (theorem) -------------------------------------------------------------
\usepackage[thmmarks,amsmath,hyperref]{ntheorem}
\usepackage[capitalise,nameinlink]{cleveref}

% Numbered Statements
\theoremstyle{change}
\theoremindent\parindent
\theorembodyfont{\itshape}
\theoremheaderfont{\bfseries\boldmath}
\newtheorem{theorem}{Theorem.}[section]
\newtheorem{lemma}[theorem]{Lemma.}
\newtheorem{corollary}[theorem]{Corollary.}
\newtheorem{proposition}[theorem]{Proposition.}

% Claim environment
\theoremstyle{plain}
\theorempreskip{0.2cm}
\theorempostskip{0.2cm}
\theoremheaderfont{\scshape}
\newtheorem{claim}{Claim}
\renewcommand\theclaim{\Roman{claim}}
\AtBeginEnvironment{theorem}{\setcounter{claim}{0}}

% Un-numbered Statements
\theorempreskip{0.1cm}
\theorempostskip{0.1cm}
\theoremindent0.0cm
\theoremstyle{nonumberplain}
\theorembodyfont{\upshape}
\theoremheaderfont{\bfseries\itshape}
\newtheorem{definition}{Definition.}
\theoremheaderfont{\itshape}
\newtheorem{example}{Example.}
\newtheorem{remark}{Remark.}

% Proof / solution environments
\theoremseparator{}
\theoremheaderfont{\hspace*{\parindent}\scshape}
\theoremsymbol{$//$}
\newtheorem{solution}{Sol'n}
\theoremsymbol{$\blacksquare$}
\theorempostskip{0.4cm}
\newtheorem{proof}{Proof}
\theoremsymbol{}
\newtheorem{nmproof}{Proof}

% Format references
\crefformat{equation}{(#2#1#3)}
\Crefformat{theorem}{#2Thm. #1#3}
\Crefformat{lemma}{#2Lem. #1#3}
\Crefformat{proposition}{#2Prop. #1#3}
\Crefformat{corollary}{#2Cor. #1#3}
\crefformat{theorem}{#2Theorem #1#3}
\crefformat{lemma}{#2Lemma #1#3}
\crefformat{proposition}{#2Proposition #1#3}
\crefformat{corollary}{#2Corollary #1#3}


% macros (algebra) -------------------------------------------------------------
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\chr}{char}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Nil}{Nil}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Tor}{Tor}

% Lagrange symbol
\newcommand{\lgs}[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}

% Quotient (larger in display mode)
\newcommand{\quot}[2]{\mathchoice{\left.\raisebox{0.14em}{$#1$}\middle/\raisebox{-0.14em}{$#2$}\right.}
                                 {\left.\raisebox{0.08em}{$#1$}\middle/\raisebox{-0.08em}{$#2$}\right.}
                                 {\left.\raisebox{0.03em}{$#1$}\middle/\raisebox{-0.03em}{$#2$}\right.}
                                 {\left.\raisebox{0em}{$#1$}\middle/\raisebox{0em}{$#2$}\right.}}


% macros (analysis) ------------------------------------------------------------
\DeclareMathOperator{\M}{{\mathcal{M}}}
\DeclareMathOperator{\B}{{\mathcal{B}}}
\DeclareMathOperator{\ps}{{\mathcal{P}}}
\DeclareMathOperator{\pr}{{\mathbb{P}}}
\DeclareMathOperator{\E}{{\mathbb{E}}}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}

\renewcommand{\Re}{\ensuremath{\operatorname{Re}}}
\renewcommand{\Im}{\ensuremath{\operatorname{Im}}}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}


% file-specific preamble -------------------------------------------------------
\newcommand{\defname}[1]{{\textit{(#1)}:}}
\newcommand{\exname}[1]{{\textit{#1}:}}
\newcommand{\defn}[1]{{\boldmath\bfseries #1}}
% \usepackage{therefore}
\newcommand{\TODO}[1]{[\textit{\textbf{TODO: #1}}]}
\newcommand{\NOTE}[1]{[\textit{\textbf{NOTE: #1}}]}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator{\ext}{ext}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Pol}{Pol}
\newcommand{\bdim}{\ensuremath{\dim_B}}
\newcommand{\ubdim}{\ensuremath{\overline{\dim}_B}}
\newcommand{\lbdim}{\ensuremath{\underline{\dim}_B}}
\newcommand{\cwx}{\ensuremath{\overline{\operatorname{conv}}^{w^*}\,}}
\newcommand{\idc}{\mathbf{1}}
\newcommand{\FA}{\ensuremath{\operatorname{F}\!\operatorname{A}}}
\newcommand{\cw}{\ensuremath{\overline{\operatorname{conv}}\,}}

% Tons of notation:
% \newcommand{\Lip}[1]{\ensuremath{\operatorname{Lip}_{\F}(#1)}}
\newcommand{\Lipspace}{\ensuremath{\operatorname{Lip}_{\F}(X,d)}}


\newcommand{\lp}[1]{\ensuremath{\ell^{#1}}}
\newcommand{\csn}{\ensuremath{\mathbf{c}}}
\newcommand{\csz}{\ensuremath{\mathbf{c}_0}}
\newcommand{\lpspace}[1]{\ensuremath{\ell^{#1}_{\F}}}
\newcommand{\Lp}[1]{\ensuremath{L^{#1}_{\F}}}
% \newcommand{\Lpm}{\ensuremath{L^{#1}_{\F}(X,\mathcal{M},\mu)}}
\DeclareMathOperator{\Lip}{Lip}
\newcommand{\lbr}[1]{\ensuremath{\left[#1\right]}}
\newcommand{\inr}[1]{\ensuremath{\left(#1\right)}}


% constants --------------------------------------------------------------------
\newcommand{\subject}{Martingales and Stochastic Calculus}
\newcommand{\semester}{Winter 2020}


% formatting -------------------------------------------------------------------
% Fonts
\usepackage{kpfonts}
\usepackage{dsfont}

% Adjust numbering
\numberwithin{equation}{section}
\counterwithin{figure}{section}
\counterwithout{section}{chapter}
\counterwithin*{chapter}{part}

% Footnote
\setfootins{0.5cm}{0.5cm} % footer space above
\renewcommand*{\thefootnote}{\fnsymbol{footnote}} % footnote symbol

% Table of Contents
\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand*{\cftchaptername}{Chapter } % Place 'Chapter' before roman
\setlength\cftchapternumwidth{4em} % Add space before chapter name
\cftpagenumbersoff{chapter} % Turn off page numbers for chapter
\maxtocdepth{subsection} % table of contents up to section

% Section / Subsection headers
\setsecnumdepth{subsection} % numbering up to and including "subsection"
\newcommand*{\shortcenter}[1]{%
    \sethangfrom{\noindent ##1}%
    \Large\boldmath\scshape\bfseries
    \centering
\parbox{5in}{\centering #1}\par}
\setsecheadstyle{\shortcenter}
\setsubsecheadstyle{\large\scshape\boldmath\bfseries\raggedright}

% Chapter Headers
\chapterstyle{verville}

% Page Headers / Footers
\copypagestyle{myruled}{ruled} % Draw formatting from existing 'ruled' style
\makeoddhead{myruled}{}{}{\scshape\subject}
\makeevenfoot{myruled}{}{\thepage}{}
\makeoddfoot{myruled}{}{\thepage}{}
\pagestyle{myruled}
\setfootins{0.5cm}{0.5cm}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

% Titlepage
\title{\subject}
\author{Alex Rutar\thanks{\itshape arutar@uwaterloo.ca}\\ University of Waterloo}
\date{\semester\thanks{Last updated: \today}}

\begin{document}
\pagenumbering{gobble}
\hypersetup{pageanchor=false}
\maketitle
\newpage
\frontmatter
\hypersetup{pageanchor=true}
\tableofcontents*
\newpage
\mainmatter


% main document ----------------------------------------------------------------
\chapter{Stochastic Calculus}
\section{Measure Theory for Probability}
\begin{definition}
    Given a measure space $(\Omega,\mathcal{F},\pr)$, a measurable function $X:\Omega\to\R$ is called a \defn{random variable}.
    If $X$ is a random variable, then we define the \defn{distribution} of $X$ to be the measure Borel measure $\mu$ on $\R$ given by $\mu(E)=\pr(X^{-1}(E))$.
\end{definition}
\subsection{Conditional Expectation}
\begin{theorem}[Kolmogorov]
    Suppose $X\in L^1(\Omega,\mathcal{F},\pr)$ and $\mathcal{G}\subset\mathcal{F}$ is a sub-$\sigma$-algebra.
    Then there exists some $Z\in L^1(\Omega,\mathcal{G},\pr)$ such that for each $A\in\mathcal{G}$
    \begin{equation*}
        \int_A Xd\pr=\int_A Zd\pr.
    \end{equation*}
    Moreover, if $\tilde Z\in L^1(\Omega,\mathcal{G},\pr)$ satisfies the above constraint, then $\pr\{x\in\Omega:\tilde Z(x)=Z(x)\}=1$.
\end{theorem}
\begin{definition}
    In the context of the above theorem, we write $Z=\E[X|\mathcal{G}]$ and call $\E[X|\mathcal{G}]$ a \defn{conditional expectation} with respect to $\mathcal{G}$.
    Certainly $\E[X|\mathcal{G}]$ nee not be pointwise unique.
\end{definition}
\begin{theorem}[Properties of Conditional Expectation]
    Suppose $X,Y$ are random variables on $(\Omega,\mathcal{F},\pr)$ with $X\in L^1(\Omega,\mathcal{F},\pr)$.
    Let $\mathcal{G}\subset\mathcal{F}$ be a sub-$\sigma$-algebra.
    Then
    \begin{enumerate}[nl]
        \item $\E[X|\mathcal{G}]\geq 0$ a.s. whenever $X\geq 0$ a.s.
        \item For $\alpha,\beta\in\R$ and $Y\in L^1(\Omega,\mathcal{F},\pr)$, $\E[\alpha X+\beta Y|\mathcal{G}]=\alpha\E[X|\mathcal{G}]+\beta\E[Y|\mathcal{G}]$ a.s.
        \item If $XY\in L^1(\Omega,\mathcal{F},\pr)$ and $Y$ is $\mathcal{G}-$measurable, then $\E[XY|\mathcal{G}]=Y\E[X|\mathcal{G}]$
        \item If $\mathcal{H}\subset\mathcal{G}$ is a $\sigma$-algebra, then $\E[\E[X|\mathcal{G}]|\mathcal{H}]=\E[X|\mathcal{H}]$ a.s.
        \item $\E[\E[X|\mathcal{G}]]=\E[X]$
        \item If $\mathcal{H}$ is a $\sigma$-algebra which is independent of the $\sigma-$algebra $\sigma\{\sigma\{X\},\mathcal{G}\}$, then $\E[X|\sigma\{\mathcal{G},\mathcal{H}\}]=\E[X|\mathcal{G}]$.
            In particular, $\E[X|\mathcal{G}]=\E[X]$ a.s. whenever $\sigma\{X\}$ and $\mathcal{H}$ are independent.
    \end{enumerate}
\end{theorem}
\subsection{Stochastic Processes}
\begin{definition}
    A \defn{stochastic process} $X=\{X_t\}_{t\in \mathcal{T}}$ is a collection of random variables defined on some probability space $(\Omega,\mathcal{F},\pr)$.
\end{definition}
Typically, we assume that $\mathcal{T}=\Z_{\geq 0}$ or $\mathcal{T}=\R_{\geq 0}$ and equip $\mathcal{T}$ with the order topology
Intuitively, we expect $t$ to be a discrete or continuous time parameter.
Given some $\omega\in\Omega$, the map $t\mapsto X_t(\omega)$ is called a \defn{realization} or \defn{path} of this process.
One of the goals of this section is to treat $\{X_t\}_{t\geq 0}$ as a random element in some path space, equipped with a proper $\sigma-$algebra and probability.

\begin{definition}
    We say that a stochastic process is \defn{$L^p$} if $X_t$ is $L^p$ for each $t\in\mathcal{T}$.
    Then we say that a stochastic process is \defn{uniformly $L^p$} if there exists some $B\in\R$ so that $\int_\Omega|X_t|^pd\pr\leq B$ for each $t\in\mathcal{T}$.
\end{definition}
Consider $X_t(\omega)$ as a function $X:\mathcal{T}\times\Omega\to \R$ equipped with the product $\sigma-$algebra.
\begin{definition}
    The \defn{distribution} of a stochastic process is the collection of all its finite-dimensional distributions, i.e. the collection of all the distributions of $\{X_{t_1},\ldots,X_{t_k}\}$ for and $k\in\N$ and $t_i\in \mathcal{T}$.
\end{definition}
There are a number of ways to say that two processes $X$ and $Y$ are equivalent, which we organize in decreasing order of strength.
\begin{definition}
    Let $X=\{X_t\}_{t\in \mathcal{T}}$ and $Y=\{Y_t\}_{t\in \mathcal{T}}$ be stochastic processes.
    \begin{itemize}[nl]
        \item We say that $X$ and $Y$ are called \defn{indistinguishable} if almost all their sample paths agree; in other words,
            \begin{equation*}
                \pr(\omega\in\Omega:X_t(\omega)=Y_t(\omega)\text{ for all }t\in \mathcal{T})=1.
            \end{equation*}
        \item We say that $Y$ is a \defn{modification} of $X$ if for each $t\in \mathcal{T}$ we have $\pr(\omega\in\Omega:X_t(\omega)=Y_t(\omega))=1$.
        \item Finally, $X$ and $Y$ are said to have the \defn{same distribution} if all the finite dimensional distributions agree.
            In other words, if for all $n\in\N$ and $t_1<\cdots<t_n$, with $t_i\in \mathcal{T}$, we have $(X_{t_1},\ldots,X_{t_n})\overset{d}{=}(Y_{t_1},\ldots,Y_{t_n})$.
    \end{itemize}
\end{definition}
\begin{example}
    Let $X$ be a continuous stochastic process and $N$ a Poisson point process on $[0,\infty)$.
    Then define
    \begin{equation*}
        Y_t :=
        \begin{cases}
            X_t &: t\notin N\\
            X_t+1 &:t\in N
        \end{cases}
    \end{equation*}
    Thus $\pr(X_t=Y_t)=1$ for all $t$, so $X$ is a modification of $Y$.
    However, $\pr(X_t=Y_t,t\geq 0)=0$, so that $X$ and $Y$ are not indistinguishable.
\end{example}
A filtration formalizes the idea of ``information acquired over time''.
\begin{definition}
    Let $(\Omega,\mathcal{F},\pr)$ be a probability space.
    A \defn{filtration} is a non-decreasing family $\{\mathcal{F}_t\}_{t\in \mathcal{T}}$ of sub-$\sigma$-algebras of $\mathcal{F}$ so that $\mathcal{F}_s\subseteq\mathcal{F}_t\subseteq\mathcal{F}$ for any $s<t$ in $\mathcal{T}$.
    We write $F_\infty=\sigma(\bigcup_{t\in \mathcal{T}}\mathcal{F}_t)$.
\end{definition}
Let $\{X_t\}_{t\in \mathcal{T}}$ be a stochastic process.
\begin{definition}
    The \defn{filtration generated by $\{X_t\}_{t\in \mathcal{T}}$} is $\{\sigma(\{X_s:s\leq t\})\}_{t\in \mathcal{T}}$.
    In other words $\mathcal{F}_t$ is the smallest $\sigma$-algebra which makes $X_s$ measurable for all $s\leq t$ in $\mathcal{T}$.
    A stochastic process $\{X_t\}_{t\in \mathcal{T}}$ is called \defn{adapted} to a filtration $\{\mathcal{F}_t\}_{t\in \mathcal{T}}$ if $X_t$ is $\mathcal{F}_t$-measurable for every $t\in \mathcal{T}$.
\end{definition}
Clearly, the filtration generated by $\{X_t\}_{t\in \mathcal{T}}$ is the smallest filtration which makes $(X_t)_{t\in \mathcal{T}}$ adapted.
\begin{definition}
    A filtration $\{\mathcal{F}_t\}_{t\in \mathcal{T}}$ is said to satisfy the ``usual condition'' if
    \begin{enumerate}[nl,r]
        \item it is right-continuous: $\lim_{s\to t^+}:=\bigcap_{s>t}\mathcal{F}_s=\mathcal{F}_t$, and
        \item $\mathcal{F}_0$ contains all the $\pr-$null events in $\mathcal{F}$.
    \end{enumerate}
\end{definition}
\section{Martingale Theory}
\subsection{Stopping Times}
Consider a filtered space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in \mathcal{T}},\pr)$, i.e. a probability space equipped with a filtration.
\begin{definition}
    A random time $N:\Omega\to \mathcal{T}$ is called a \defn{stopping time} if $N^{-1}([0,t])\in\mathcal{F}_t$ for each $t\in \mathcal{T}$.
\end{definition}
The philosophy here is that we know that a stopping time happens when it happens.
\begin{example}
    \begin{enumerate}[nl,r]
        \item Constants are trivial stopping times.
        \item Last hit a constant before some fixed bound is not a stopping time
    \end{enumerate}
\end{example}
\begin{proposition}
    If $T,S$ are stopping times, $\min\{T,S\}$, $\max\{T,S\}$, and $T+S$ are stopping times.
\end{proposition}
\begin{proof}
    \begin{itemize}[nl]
        \item We have $\min\{T,S\}^{-1}([0,t])=T^{-1}([0,t])\cap S^{-1}([0,t])\in\mathcal{F}_t$.
        \item Similarly, we have $\max\{T,S\}^{-1}([0,t])=T^{-1}([0,t])\cup S^{-1}([0,t])\in\mathcal{F}_t$.
        \item For $T+S$, we have that
            \begin{align*}
                (T+S)^{-1}((t,\infty))&=\bigl(T^{-1}(\{0\})\cap S^{-1}((t,\infty))\bigr)\cup S^{-1}((t,\infty))\\
                                      &\agspace \cup\bigl(T^{-1}((0,t])\cap (T+S)^{-1}((t,\infty))\bigr)
            \end{align*}
            where $T^{-1}(\{0\})\cap S^{-1}((t,\infty))\in\mathcal{F}_t$ and $S^{-1}((t,\infty))\in\mathcal{F}_t$.
            To finish the proof, since $T$ has a countable dense subset $Q$, we have
            \begin{equation*}
                T^{-1}((0,t])\cap (T+S)^{-1}((t,\infty))=\bigcup_{r\in Q\cap(0,t)}S^{-1}((r,t])\cap T^{-1}((t-r,\infty))
            \end{equation*}
            where the expressions in the union are certainly $\mathcal{F}_t$-measurable.
    \end{itemize}
\end{proof}
\begin{definition}
    The \defn{$\sigma-$algebra generated by a stopping time $T$} is the set of all events $A$ for which $A\cap T^{-1}([0,t])\in\mathcal{F}_t$ for every $t\in \mathcal{T}$.
\end{definition}
Intuitively, this is the information you collect until the stopping time.
Note that this $\sigma$-algebra is not the same as the $\sigma$-algebra generated by the random variable $T$.
It is left as an exercise to the reader to verify that the above collection is indeed a $\sigma$-algebra.

We write $X_{T\wedge t}$ is a random variable evaluated at time $T\wedge t$ (or $T$); in other words, $(X_{T\wedge t})(\omega)=X_{T\wedge t}(\omega)$.
Then $\{X_{T\wedge t}\}_{t\in\mathcal{T}}$, or $X^T$, is a stochastic process stopped at time $t$.
\subsection{Doob's Upcrossing Inequality}
\begin{definition}
    Consider a filtered probability space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in\mathcal{T}},\pr)$.
    A $\{\mathcal{F}_t\}_{t\in \mathcal{T}}-$adapted proces $\{X_t\}_{t\in \mathcal{T}}$ is said to be a \defn{submartingale} if
    \begin{enumerate}[nl,r]
        \item for all $t\in \mathcal{T}$, $X_t\in L^1(\Omega,\mathcal{F},\ps)$, and
        \item for all $s<t$ where $s,t\in \mathcal{T}$,
            \begin{equation*}
                \E(X_t | \mathcal{F}_s)\geq X_s
            \end{equation*}
            almost surely.
    \end{enumerate}
    and is said to be a \defn{supermartingale} if condition (ii) above is replaced with
    \begin{enumerate}[nl,r]
        \item[(ii')] for all $s<t$ where $s,t\in \mathcal{T}$,
            \begin{equation*}
                \E(X_t | \mathcal{F}_s)\leq X_s
            \end{equation*}
            almost surely.
    \end{enumerate}
    Then $\{X_t\}_{t\in \mathcal{T}}$ is a \defn{martingale} if it is both a submartingale and supermartingale.
\end{definition}
If $X$ is a submartingale and $0\leq s<t$ are fixed times, then $\E(X_0)\leq \E(X_s)\leq \E(X_t)$.
Similar statements hold in the (super)martingale cases as well.
One of the goals of martingale theory is to extend these results with respect to stopping times, rather than with respect to fixed times.

\begin{definition}
    Let $X=\{X_n\}_{n\in\Z^+}$ be a discrete time process and fix levels $a<b$ with $a,b\in\R$.
    Then the \defn{number of upcrossings} of $[a,b]$ by $X$ before time $N$ with respect to the event $\omega\in\Omega$, denoted by $U_N^X([a,b],\omega)$, is the maximal $k\in\Z^+$ such that there exists times $0\leq s_1<t_1<s_2<t_2<\cdots<s_k<t_k\leq N$ such that for all $i$, $X_{s_i}(\omega)\leq a$ and $X_{t_i}(\omega)\geq b$.
\end{definition}
Note that $U_N^X([a,b]):\Omega\to\Z^+$ given by $\omega\mapsto U_N^X([a,b],\omega)$ is a random variable.
\begin{definition}
    A process $C=\{C_n\}_{n\in\Z^+}$ is called \defn{previsible} if $C_n$ is $\mathcal{F}_{n-1}-$measurable for all $n\geq 1$.
    Suppose in addition that $\{X_n\}_{n\in\Z^+}$ is a discrete time process.
    Then the \defn{martingale transform} of $X$ by $C$, denoted by $C\cdot X$, is defined by
    \begin{equation*}
        (C\cdot X)_n=
        \begin{cases}
            \sum_{k=1}^n C_k(X_k-X_{k-1}) &: n>0\\
            0 &: n=0
        \end{cases}
    \end{equation*}
\end{definition}
\begin{lemma}\label{l:previs}
    Suppose $C$ is a bounded or $L^2$ previsible process.
    Then
    \begin{enumerate}[nl,r]
        \item Let $C$ be non-negative and $X$ a supermartingale.
            Then $C\cdot X$ is a supermartingale which is null at 0.
        \item Let $X$ be a martingale.
            Then $C\cdot X$ is a martingale which is null at 0.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We first treat the case where $C$ is bounded.
    \begin{enumerate}[nl,r]
        \item We have by properties of conditional expectation and non-negativity of $C$
            \begin{align*}
                \E[(C\cdot X)_n-(C\cdot X)_{n-1}|\mathcal{F}_{n-1}] &= \E[C_n(X_n-X_{n-1})|\mathcal{F}_{n-1}]\\
                                                                    &= C_n\cdot \E[X_n-X_{n-1}|\mathcal{F}_{n-1}]\\
                                                                    &\leq 0
            \end{align*}
            where the second line follows since $C$ is previsible.
            Integrability follows immediately since $C$ is bounded and $X_n$ is $L^1$ for each $n\in\Z^+$.
        \item Consider $C+k$ where $k$ is a constant and $k\geq|C_n(w)|$ for all $n$ and $w$.
        \item Similar, but the integrability is now guaranteed by Hölder's inequality.
    \end{enumerate}
    % When $C$ is $L^2$, by Hölder's inequality, we have integrability of $C\cdot X$ given by
    % \begin{equation*}
    % \end{equation*}
\end{proof}
We now have the following result:
\begin{proposition}[Doob's Upcrossing Inequality]
    \begin{enumerate}[nl,r]
        \item Let $X$ be a supermartingale.
            Then
            \begin{equation*}
                (b-a)\cdot\E[U_N^X([a,b])]\leq\E[(X_N-a)^-].
            \end{equation*}
        \item Let $X$ be a submartingale.
            Then
            \begin{equation*}
                (b-a)\cdot\E[U_N^X([a,b])]\leq\E[(X_N-a)^+].
            \end{equation*}
\end{proposition}
\begin{proof}
    We prove (i); the proof of (ii) is analgous.
    Define a process $\{C_n\}_{n\in\Z^+}$ by
    \begin{align*}
        C_0&:= 0\\
        C_1&:=\chi_{X_0^{-1}((-\infty,a))}\\
        C_n&:=\chi_{C_{n-1}^{-1}(\{1\})}\cdot\chi_{X_{n-1}^{-1}((-\infty,b])}+\chi_{C_{n-1}^{-1}(\{0\})}\cdot\chi_{X_{n-1}^{-1}((-\infty,a))}.
    \end{align*}
    Certainly $C$ is non-negative and bounded; previsibility follows since $C_n$ is a characteristic function depending only on preimages of $X_k$ for $k<n$.
    Thus set $Y=C\cdot X$, i.e. $Y_n=\sum_{k=1}^n C_k\cdot(X_k-X_{k-1})$ almost surely; by \cref{l:previs}, $Y$ is a supermartingale.

    Since each finished upcrossing increases the value of $Y$ by at least $b-a$, we have for any $\omega\in\Omega$
    \begin{equation*}
        Y_N(\omega)\geq(b-a)U_N^X([a,b],\omega)-(X_n-a)^-(\omega)
    \end{equation*}
    where $(X_N-a)^-(\omega)$ is the upper bound for the ``loss'' due to the last unfinished upcrossing.
    Since $\E(Y_1)\leq 0$, we have $\E(Y_N)\leq\E(Y_1)\leq 0$.
    Thus
    \begin{equation*}
        (b-a)\cdot\E[U_N^X([a,b])]\leq\E[(X_n-a)^-]
    \end{equation*}
    as required.
\end{proof}
\subsection{Optimal Sampling Theorems}
\begin{theorem}
    Suppose $\{X_t\}_{t\in\mathcal{T}}$ is uniformly $L^2$.
    Then $X_t$ converge in $L^2$ to a limit $X_\infty$.
\end{theorem}
\begin{proof}
    Note that we have the result for both discrete and continuous time martingales.
    By discretization, it is clear that it suffices to rove the result for discrete case.

    We have the following orthogonality between the increments of a martingale $\{X_n\}_{n=0}^\infty$: if $n_1<n_2\leq n_2<n_4$, then
    \begin{equation*}
        \E[(X_{n_2}-X_{n_1})(X_{n_4}-X_{n_3})]=0.
    \end{equation*}
    This result follows by conditioning on $\mathcal{F}_{n_3}$ and applying the law of total expectation.

    Now the proof proceeds.
    Set $Y_n:=X_n-X_{n-1}$, so
    \begin{equation*}
        \norm{X_n}_2^2=\E(X_n^2)=\sum_{i=1}^n\norm{Y_i}_2^2
    \end{equation*}
    and $\sum_{i=0}^n\norm{Y_n}_2^2\leq B$ for all $n$, so that $\sum_{i=0}^\infty\norm{Y_n}_2^2\leq B$.
    Thus $\{X_n\}_{n=0}^\infty$ is Cauchy in $L^2(\Omega,\mathcal{F},\pr)$.
\end{proof}
\begin{theorem}[Optimal Sampling for Bounded Stopping Times in Discrete Times]
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ supermartingale and $S,T$ be $\{\mathcal{F}_n\}-$stopping times such that $0\leq S\leq T\leq N$ for some constant $N<\infty$.
    Then $X_T$ is integrable and $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely.
\end{theorem}
\begin{proof}
    Notice that
    \begin{equation*}
        |X_T|\leq|X_0|+|X_1|+\cdots+|X_N|
    \end{equation*}
    so that $\E(|X_T|)<\infty$.
    To prove that $\E(X_t|\mathcal{F}_s)\leq X_s$ a.s., it suffices to prove that $\E(X_T;A):=\int_A X_Td\ps\leq\int_A X_s d\ps=:\E(X_s;A)$ for all $A\in\mathcal{F}_s$.
    Assuming this, then
    \begin{equation*}
        \E(\E(X_T|\mathcal{F}_s)-X_s;A)\leq 0
    \end{equation*}
    for all $A\in\mathcal{F}_s$, so we may take $A=A_0:=\{\E(X_T|\mathcal{F}_s)-X_s>0\}$, so that $\pr(A_0)=0$.

    Let's prove the required statement.
    First note that
    \begin{equation*}
        \sum_{n=1}^N \chi_{\{S<n\leq T\}}(X_n-X_{n-1})=X_T-X_S
    \end{equation*}
    and taking expectation over $A$ on both sides
    \begin{align*}
        \E(X_T-X_S;A) &= \sum_{n=1}^N\E(\chi_{\{S<n\leq T\}}(X_n-X_{n-1});A)\\
                      &= \sum_{n=1}^N\E(X_n-X_{n-1};A\cap\{S<n\leq T\})
    \end{align*}
    But $A\cap\{S<n\leq T\}=A\cap\{S\leq n-1\}\cap\{n-1<T\}\in\mathcal{F}_{n-1}$.
    Thus
    \begin{equation*}
        \E(X_n-X_{n-1}; A\cap\{S<n\leq T\})=\E(\E(X_n-X_{n-1}|\mathcal{F}_{n-1});A\cap\{S<n\leq T\})\leq 0
    \end{equation*}
    so the required statement holds.
\end{proof}
\begin{definition}
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ a supermartingale.
    We say that $\{X_n\}_{n=0}^\infty$ is \defn{closed} by a random variable $X_\infty$ if $X_\infty$ is $\mathcal{F}_\infty-$measurable and $X_n\geq\E(X_\infty|\mathcal{F}_n)$ almost surely for all $n=0,1,\ldots$.
\end{definition}
Similar statements hold with $X_n\leq\E(X_\infty|\mathcal{F}_n)$ for a submartingale, or equality with a martingale.
\begin{proposition}
    Suppose that $\{X_n\}_{n=0}^\infty$ is a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ a non-negative supermartingale, and $X_\infty=0$.
    If $S,T:\Omega\to\overline{\Z^+}$ are $\{\mathcal{F}_n\}-$stopping times, $S\leq T$, then
    \begin{enumerate}[nl]
        \item $\E(X_T)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[nl]
        \item Note that $X_T\leq\liminf_{n\to\infty}X_{T\wedge n}$ where $T\wedge n$ and $0$ are two bounded stopping times.
            Thus $\E(X_{T\wedge n})\leq\E(X_0)$ for all $n=0,1,\ldots$.
            Thus by Fatou's lemma
            \begin{align*}
                \E(X_T)&\leq\E(\liminf_{n\to\infty} X_{T\wedge n})\leq\liminf_{n\to\infty}\E(X_{T\wedge n})\\
                       &\leq\E(X_0)<\infty.
            \end{align*}
        \item Let $A\in\mathcal{F}_s$.
            For $n=0,1,\ldots,$,
            \begin{align*}
                \E(X_T;A\cap\{T\leq n\}) &= \E(X_{T\wedge n};A\cap\{T\leq n\})\\
                                         &\leq \E(X_{T\wedge n};A\cap\{S\leq n\})\\
                                         &\leq \E(X_{S\wedge n}; A\cap\{S\leq n\})
            \end{align*}
            Note that $S\wedge n$ and $T\wedge n$ are two bounded stopping times with $S\wedge n\leq T\wedge n$.
            Also, $A\cap\{S\leq n\}\in\mathcal{F}_{S\wedge n}$.
            Then apply the optimal sampling theorem for bounded stopping times.
            By the monotone convergence theorem,
            \begin{equation*}
                \lim_{n\to\infty}\E(X_T;A\cap\{T\leq n\})
            \end{equation*}
            and similarly for $S$.
            Thus
            \begin{equation*}
                \E(X_T;A\cap\{T<\infty\})\leq\E(X_S:A\cap\{S<\infty\})
            \end{equation*}
            so that
            \begin{equation*}
                \E(X_T;A\cap\{T=\infty\})=\E(X_S:A\cap\{S=\infty\})=0
            \end{equation*}
            and $\E(X_T;A)\leq\E(X_S;A)$.
            Since this holds for all $A\in\mathcal{F}_s$, $\E(X_T:\mathcal{F}_s)\leq X_s$.
    \end{enumerate}
\end{proof}
\begin{lemma}
    Let $\{M_n\}_{n=0}^\infty$ be a martingale closed by $M_\infty$.
    Let $T:\Omega\to\overline{\Z^+}$ be a stopping time.
    Then $M_T=\E(M_\infty|\mathcal{F}_T)$.
\end{lemma}
\begin{proof}
    First assume $M_\infty\geq 0$.
    For any $A\in\mathcal{F}_T$,
    \begin{align*}
        E(M_T;A)&=\sum_{n=0}^\infty\E(M_n;A\cap\{T=n\})\\
                &=\sum_{n=0}^\infty\E(M_\infty,A\cap\{T=n\})\\
                &= \E(M_\infty;A).
    \end{align*}
    For the general case, decompose $M_\infty$ into positive and negative parts.
\end{proof}
\begin{theorem}[Optimal Sampling for Closed Supermartingales in Discrete Time]
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ supermartingale closed by $X_\infty$.
    Let $S,T:\Omega\to\overline{\Z^+}$ be two $\{\mathcal{F}_n\}_{n=0}^\infty$ stopping times with $S\leq T$.
    Then
    \begin{enumerate}[nl]
        \item $\E(|X_T|)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[nl]
        \item Define $M_n=\E(X_\infty|\mathcal{F}_n)$ and $A_n=X_n-M_n$ for $n=0,1,\ldots,\infty$.
            Since $\{X_n\}_{n=0}^\infty$ is a supermartingale closed by $X_\infty$, $A_n\geq 0$, $A_\infty=0$, and for $m\leq n$,
            \begin{align*}
                \E(A_n|\mathcal{F}_m)&=\E(X_n-\E(X_\infty|\mathcal{F}_n)|\mathcal{F}_m)\\
                                     &\leq X_m-\E(X_\infty|\mathcal{F}_m)\\
                                     &= A_m
            \end{align*}
            Thus $\{A_n\}_{n=0}^\infty$ is a non-negative supermartingale with $A_\infty=0$.

            One can prove that $\{M_n\}_{n=0}^\infty$ is a uniformly integrable martingale, i.e. $\lim_{c\to\infty}\sup_{n\in\N}\E(|X_n|;|X_n|>c)=0$.

            By the previous proposition, $\E(A_T)<\infty$.
            On the other hand, $\E(|M_T|)<\infty$ by definition of the $\{M_n\}$.
            Thus $\E(|X_T|)<\infty$.
        \item Apply the previous lemma so $\E(M_T|\mathcal{F}_s)=\E(\E(M_\infty|\mathcal{F}_T)\mathcal{F}_S)=\E(M_\infty|\mathcal{F}_s)=M_s$ by the optimal sampling theorem for closed martingales.
            Meanwhile, as $\{A_n\}_{n=0}^\infty$ is a non-negative supermartingale with $A_\infty=0$, $\E(A_T|\mathcal{F}_s)\leq A_s$ almost surely.
            Thus $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely.
    \end{enumerate}
\end{proof}
To prepare for the optimal sampling theorem for closed supermartingales in continuous time, we introduce one more discrete time notion:
\begin{definition}
    Denote by $\Z^-=\{z\in\Z:z\leq 0\}$.
    A \defn{negatively indexed supermartingale} $\{X_n\}_{n\in\Z^-}$ and $\{\mathcal{F}_n\}_{n\in\Z^-}$ where $\mathcal{F}_m\subseteq\mathcal{F}_n$ for $m\leq n$.
    Then $X_n\in L^1(\Omega,\mathcal{F},\pr)$, $\{X_n\}$ is adapted and $\E(X_n|\mathcal{F}_m)\leq X_m$.
\end{definition}
\begin{theorem}
    Let $\{X_n\}_{n\in\Z^-}$ be a negatively indexed supermartingale such that $\sup_{n\in\Z^-}\E(X_n)<\infty$.
    Then $\{X_n\}_{n\in\Z^-}$ is uniformly $L^1$.
\end{theorem}
\begin{proof}
    Fix $a=\sup_{n\in\Z^-}\E(X_n)$.
    For any $\epsilon>0$, get $N(\epsilon)\in\Z^-$ such that $\E(X_{N(\epsilon)})>a-\epsilon$ for $N\leq N(\epsilon)$.
    Since $X$ is a supermartingale, $0\leq\E(X_n)-\E(X_{N(\epsilon)})\leq\epsilon$ for all $n\leq N(\epsilon)$.
    For $c>0$, note that
    \begin{equation*}
        |x|\chi_{|X|>c}=-x\chi_{X<-c}-X\chi_{X\leq x}+x.
    \end{equation*}
    Since $\{X_n\}$ is a supermartingale, for $n\leq N(\epsilon)$, $\E(X_{N(\epsilon)}|\mathcal{F}_n)\leq X_n$ and $\{X_n<-c\}\in\mathcal{F}_n$.
    Thus $\E(X_n;X_n<-c)\geq\E(X_{N(\epsilon)};X_n<-c)$.
    Similarly, $\E(X_n;X_n\leq c)\geq\E(X_{N(\epsilon)};X_n\leq c)$.
    Moreover, $\E(X_n)\leq\E(X_{N(\epsilon)})+\epsilon$, so $\E(|X_n|;X_n\leq c)\leq\E(X_{N(\epsilon)};X_n\leq c)+\epsilon$ for all $N\leq N(\epsilon)$.
    For this $N(\epsilon)$, there exists $\delta(\epsilon)>0$ such that $\E(X_{N(\epsilon)};A)<\epsilon$ for all $A\in\mathcal{F}$ and $\pr(A)<\delta(\epsilon)$.

    Since $X$ is a supermartingale, $\{-2X_n^-\}_{n\in\Z^-}$ is also a supermartingale.
    To see this, define $f(x)=2\min\{x,0\}$, so $f$ is increasing and concave.
    But then for $M\leq n$, by Jensen's inequality,
    \begin{align*}
        \E(f(X_n)|\mathcal{F}_m) &\leq f(\E(X_n|\mathcal{F}_m))\leq f(X_m)
    \end{align*}
    and $-2X_n^-=f(X_n)$ is thus a supermartingale, so $\E(-2X_n^-)\geq\E(-2X_0^-)$.
    We also have $\E(X_n)\leq a$ where $a$ is defined as above.
    Thus since $|X_n|=X_n+2X_n^-$, we have
    \begin{equation*}
        \E(|X_n|)\leq a-\E(-2X_0^-)=a+2\E(X_0^-)<\infty.
    \end{equation*}

    Now, choose $c$ so that $c\cdot\delta(\epsilon)>a+2\E(X_0^-)\geq\E(|X_n|)$.
    Then by Markov's ineuality, $\pr(|X_n|>c)\leq\delta(\epsilon)$.
    Thus $\E(|X_{N(\epsilon)}|;|X_n|>c)<\epsilon$ for $n\in\Z^-$ so that $\E(|X_m|;|X_n|>c)<2\epsilon$ for all $n<N(\epsilon)$.
    But there are only finitely many terms with $n>N(\epsilon)$, so we are done.
\end{proof}
\begin{theorem}[Optimal Sampling Theorem for Closed Supermartingales]
    Let $\{X_t\}_{t\in\R^+}$ be an $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in\R^+},\pr)$ supermartingale which is right continuous and closed.
    Let $S,T:\Omega\to\overline{\R^+}$ be two stopping times with $S\leq T$.
    Then
    \begin{enumerate}[nl,r]
        \item $\E(|X_T|)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$ a.s.
    \end{enumerate}
\end{theorem}
\begin{definition}
    Define random times $T_n:=2^{-n}(\lfloor 2^nT\rfloor+1)$.
    Then the $T_n$ are decreasing stopping times with $T=\lim T_n$ pointwise from above.
    For each $n$, $\{X_{m2^{-n}}\}_{n\in\Z^+}$ is a discrete-time closed supermartngale.
    Thus $\E(X_{T_n-1}|\mathcal{F}_{T_n})\leq X_{T_n}$ a.s. (optimal sampling theorem for closed supermartingale in discrete time).
    Set $Y_n=X_{T_n-1}$, so $\{Y_n\}_{n=-1,-2,\ldots}$ is a negatively indexed supermartingale and $\E(Y_n)=\E(X_{T-n-1})\leq\E(X_0)<\infty$.
    Thus by the previous result, $\{X_{T_n}\}_{n\in\Z^+}$ is uniformly $L^1$.
    Since $T_n\to T$ from above and $X_{T_n}\to X_T$ a.s., uniform integrability gives that $X_{T_n}\to X_T$ in $L^1$.
    Thus $\E(|X_T|)=\lim_{n\to\infty}\E(|X_{T_n}|)<\infty$.
    Similarly, define $S_n=2^{-n}(\lfloor 2^nS\rfloor+1)$.
    Both $S_n$ and $T_n$ can be regarded as discrete-time stopping times.
    Thus by the discrete optimal sampling theorem, $\E(X_{T_n}|\mathcal{F}_{S_n})\leq X_{S_n}$ a.s.

    Now for $A\in\mathcal{F}_S\subseteq\mathcal{F}_{S_n}$, $\E(X_{T_n};A)\leq\E(X_{S_n};A)$ so
    \begin{equation*}
        |\E(X_{T_n};A)-\E(X_T;A)|\;eq\E(|X_{T_n}-X_T|)\fto{n\to\infty} c
    \end{equation*}
    Also, $|\E(X_{S_n};A)-\E(X_S;A)|\to 0$.
    Therefore, as $n$ goes to infinity,
    \begin{equation*}
        \E(X_T;A)\leq\E(X_S;A)
    \end{equation*}
    for all $A\in\mathcal{F}_S$ so $\E(X_T|\mathcal{F}_S)\leq X_s$.
\end{definition}
\begin{corollary}
    Let $\{X_t\}_{t\in\R^+}$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in\R^+}$ martingale which is right continuous and closed.
    If $S,T:\Omega\to\overline{\R^+}$ are two stopping times with $S\leq T$, then $X_T$ is integrable and $|E(X_T|\mathcal{F}_S)=X_S$.
\end{corollary}
\begin{corollary}
    Same results hold for bounded $S,T$ and general right continuous martingale.
\end{corollary}
\begin{corollary}
    A right continuous adapted process $X$ is a martingale if and only if for every bounded stopping time $T$, $X_T$ is $L^1$ and $\E(X_T)=\E(X_0)$.
\end{corollary}
\begin{proof}
    The forward direction is contained in the above theorem.
    Conversely, for any $s<t$ and $A\in\mathcal{F}_s$, define $T=t\chi_{A^c}+s\chi_{A}$ which is a stopping time (exercise).
    Then $\E(X_0)=\E(X_T)=\E(X_t;A^c)+\E(X_s;A)$.
    On the other hand, $\E(X_0)=\E(X_t)=\E(X_t;A^c)+\E(X_t;A)$ so $\E(X_s;A)=\E(X_t;A)$.
    But $A\in\mathcal{F}_s$ is arbitrary, so $\E(X_t|\mathcal{F}_s)=\E(X_s)$.
\end{proof}
\end{document}
