
% header -----------------------------------------------------------------------
% Template created by texnew (author: Alex Rutar); info can be found at 'https://github.com/alexrutar/texnew'.
% version (1.13)


% doctype ----------------------------------------------------------------------
\documentclass[11pt, a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
\usepackage[protrusion=true,expansion=true]{microtype}


% packages ---------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{braket}

% Set enimitem
\usepackage{enumitem}
\SetEnumitemKey{nl}{nolistsep}
\SetEnumitemKey{r}{label=(\roman*)}

% Set tikz
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.15}
\usetikzlibrary{intersections,positioning,cd}
\usetikzlibrary{arrows,arrows.meta}
\tikzcdset{arrow style=tikz,diagrams={>=stealth}}

% Set hyperref
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{orange!50!yellow}
\colorlet{myurlcolor}{green!50!blue}

\hypersetup{
  linkcolor  = mylinkcolor!\myshade!black,
  citecolor  = mycitecolor!\myshade!black,
  urlcolor   = myurlcolor!\myshade!black,
  colorlinks = true,
}


% macros -----------------------------------------------------------------------
\DeclareMathOperator{\N}{{\mathbb{N}}}
\DeclareMathOperator{\Q}{{\mathbb{Q}}}
\DeclareMathOperator{\Z}{{\mathbb{Z}}}
\DeclareMathOperator{\R}{{\mathbb{R}}}
\DeclareMathOperator{\C}{{\mathbb{C}}}
\DeclareMathOperator{\F}{{\mathbb{F}}}

% Boldface includes math
\newcommand{\mbf}[1]{{\boldmath\bfseries #1}}

% proof implications
\newcommand{\imp}[2]{($#1\Rightarrow#2$)\hspace{0.2cm}}
\newcommand{\impe}[2]{($#1\Leftrightarrow#2$)\hspace{0.2cm}}
\newcommand{\impr}{{($\Longrightarrow$)\hspace{0.2cm}}}
\newcommand{\impl}{{($\Longleftarrow$)\hspace{0.2cm}}}

% align macros
\newcommand{\agspace}{\ensuremath{\phantom{--}}}
\newcommand{\agvdots}{\ensuremath{\hspace{0.16cm}\vdots}}

% convenient brackets
\newcommand{\brac}[1]{\ensuremath{\left\langle #1 \right\rangle}}
\newcommand{\norm}[1]{\ensuremath{\left\lVert#1\right\rVert}}
\newcommand{\abs}[1]{\ensuremath{\left\lvert#1\right\rvert}}

% arrows
\newcommand{\lto}[0]{\ensuremath{\longrightarrow}}
\newcommand{\fto}[1]{\ensuremath{\xrightarrow{\scriptstyle{#1}}}}
\newcommand{\hto}[0]{\ensuremath{\hookrightarrow}}
\newcommand{\mapsfrom}[0]{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}
 
% Divides, Not Divides
\renewcommand{\div}{\bigm|}
\newcommand{\ndiv}{%
    \mathrel{\mkern.5mu % small adjustment
        % superimpose \nmid to \big|
        \ooalign{\hidewidth$\big|$\hidewidth\cr$/$\cr}%
    }%
}

% Convenient overline
\newcommand{\ol}[1]{\ensuremath{\overline{#1}}}

% Big \cdot
\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

% Big and small Disjoint union
\makeatletter
\providecommand*{\cupdot}{%
  \mathbin{%
    \mathpalette\@cupdot{}%
  }%
}
\newcommand*{\@cupdot}[2]{%
  \ooalign{%
    $\m@th#1\cup$\cr
    \sbox0{$#1\cup$}%
    \dimen@=\ht0 %
    \sbox0{$\m@th#1\cdot$}%
    \advance\dimen@ by -\ht0 %
    \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }%
}

\providecommand*{\bigcupdot}{%
  \mathop{%
    \vphantom{\bigcup}%
    \mathpalette\@bigcupdot{}%
  }%
}
\newcommand*{\@bigcupdot}[2]{%
  \ooalign{%
    $\m@th#1\bigcup$\cr
    \sbox0{$#1\bigcup$}%
    \dimen@=\ht0 %
    \advance\dimen@ by -\dp0 %
    \sbox0{\scalebox{2}{$\m@th#1\cdot$}}%
    \advance\dimen@ by -\ht0 %
    \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }%
}
\makeatother


% macros (theorem) -------------------------------------------------------------
\usepackage[thmmarks,amsmath,hyperref]{ntheorem}
\usepackage[capitalise,nameinlink]{cleveref}

% Numbered Statements
\theoremstyle{change}
\theoremindent\parindent
\theorembodyfont{\itshape}
\theoremheaderfont{\bfseries\boldmath}
\newtheorem{theorem}{Theorem.}[section]
\newtheorem{lemma}[theorem]{Lemma.}
\newtheorem{corollary}[theorem]{Corollary.}
\newtheorem{proposition}[theorem]{Proposition.}

% Claim environment
\theoremstyle{plain}
\theorempreskip{0.2cm}
\theorempostskip{0.2cm}
\theoremheaderfont{\scshape}
\newtheorem{claim}{Claim}
\renewcommand\theclaim{\Roman{claim}}
\AtBeginEnvironment{theorem}{\setcounter{claim}{0}}

% Un-numbered Statements
\theorempreskip{0.1cm}
\theorempostskip{0.1cm}
\theoremindent0.0cm
\theoremstyle{nonumberplain}
\theorembodyfont{\upshape}
\theoremheaderfont{\bfseries\itshape}
\newtheorem{definition}{Definition.}
\theoremheaderfont{\itshape}
\newtheorem{example}{Example.}
\newtheorem{remark}{Remark.}

% Proof / solution environments
\theoremseparator{}
\theoremheaderfont{\hspace*{\parindent}\scshape}
\theoremsymbol{$//$}
\newtheorem{solution}{Sol'n}
\theoremsymbol{$\blacksquare$}
\theorempostskip{0.4cm}
\newtheorem{proof}{Proof}
\theoremsymbol{}
\newtheorem{nmproof}{Proof}

% Format references
\crefformat{equation}{(#2#1#3)}
\Crefformat{theorem}{#2Thm. #1#3}
\Crefformat{lemma}{#2Lem. #1#3}
\Crefformat{proposition}{#2Prop. #1#3}
\Crefformat{corollary}{#2Cor. #1#3}
\crefformat{theorem}{#2Theorem #1#3}
\crefformat{lemma}{#2Lemma #1#3}
\crefformat{proposition}{#2Proposition #1#3}
\crefformat{corollary}{#2Corollary #1#3}


% macros (algebra) -------------------------------------------------------------
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\chr}{char}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Nil}{Nil}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Tor}{Tor}

% Lagrange symbol
\newcommand{\lgs}[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}

% Quotient (larger in display mode)
\newcommand{\quot}[2]{\mathchoice{\left.\raisebox{0.14em}{$#1$}\middle/\raisebox{-0.14em}{$#2$}\right.}
                                 {\left.\raisebox{0.08em}{$#1$}\middle/\raisebox{-0.08em}{$#2$}\right.}
                                 {\left.\raisebox{0.03em}{$#1$}\middle/\raisebox{-0.03em}{$#2$}\right.}
                                 {\left.\raisebox{0em}{$#1$}\middle/\raisebox{0em}{$#2$}\right.}}


% macros (analysis) ------------------------------------------------------------
\DeclareMathOperator{\M}{{\mathcal{M}}}
\DeclareMathOperator{\B}{{\mathcal{B}}}
\DeclareMathOperator{\ps}{{\mathcal{P}}}
\DeclareMathOperator{\pr}{{\mathbb{P}}}
\DeclareMathOperator{\E}{{\mathbb{E}}}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}

\renewcommand{\Re}{\ensuremath{\operatorname{Re}}}
\renewcommand{\Im}{\ensuremath{\operatorname{Im}}}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}


% file-specific preamble -------------------------------------------------------
\newcommand{\defname}[1]{{\textit{(#1)}:}}
\newcommand{\exname}[1]{{\textit{#1}:}}
\newcommand{\defn}[1]{{\boldmath\bfseries #1}}
% \usepackage{therefore}
\newcommand{\TODO}[1]{[\textit{\textbf{TODO: #1}}]}
\newcommand{\NOTE}[1]{[\textit{\textbf{NOTE: #1}}]}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator{\ext}{ext}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Pol}{Pol}
\newcommand{\bdim}{\ensuremath{\dim_B}}
\newcommand{\ubdim}{\ensuremath{\overline{\dim}_B}}
\newcommand{\lbdim}{\ensuremath{\underline{\dim}_B}}
\newcommand{\cwx}{\ensuremath{\overline{\operatorname{conv}}^{w^*}\,}}
\newcommand{\idc}{\mathbf{1}}
\newcommand{\FA}{\ensuremath{\operatorname{F}\!\operatorname{A}}}
\newcommand{\cw}{\ensuremath{\overline{\operatorname{conv}}\,}}

% Tons of notation:
% \newcommand{\Lip}[1]{\ensuremath{\operatorname{Lip}_{\F}(#1)}}
\newcommand{\Lipspace}{\ensuremath{\operatorname{Lip}_{\F}(X,d)}}


\newcommand{\lp}[1]{\ensuremath{\ell^{#1}}}
\newcommand{\csn}{\ensuremath{\mathbf{c}}}
\newcommand{\csz}{\ensuremath{\mathbf{c}_0}}
\newcommand{\lpspace}[1]{\ensuremath{\ell^{#1}_{\F}}}
\newcommand{\Lp}[1]{\ensuremath{L^{#1}_{\F}}}
% \newcommand{\Lpm}{\ensuremath{L^{#1}_{\F}(X,\mathcal{M},\mu)}}
\DeclareMathOperator{\Lip}{Lip}
\newcommand{\lbr}[1]{\ensuremath{\left[#1\right]}}
\newcommand{\inr}[1]{\ensuremath{\left(#1\right)}}


% constants --------------------------------------------------------------------
\newcommand{\subject}{Martingales and Stochastic Calculus}
\newcommand{\semester}{Winter 2020}


% formatting -------------------------------------------------------------------
% Fonts
\usepackage{kpfonts}
\usepackage{dsfont}

% Adjust numbering
\numberwithin{equation}{section}
\counterwithin{figure}{section}
\counterwithout{section}{chapter}
\counterwithin*{chapter}{part}

% Footnote
\setfootins{0.5cm}{0.5cm} % footer space above
\renewcommand*{\thefootnote}{\fnsymbol{footnote}} % footnote symbol

% Table of Contents
\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand*{\cftchaptername}{Chapter } % Place 'Chapter' before roman
\setlength\cftchapternumwidth{4em} % Add space before chapter name
\cftpagenumbersoff{chapter} % Turn off page numbers for chapter
\maxtocdepth{subsection} % table of contents up to section

% Section / Subsection headers
\setsecnumdepth{subsection} % numbering up to and including "subsection"
\newcommand*{\shortcenter}[1]{%
    \sethangfrom{\noindent ##1}%
    \Large\boldmath\scshape\bfseries
    \centering
\parbox{5in}{\centering #1}\par}
\setsecheadstyle{\shortcenter}
\setsubsecheadstyle{\large\scshape\boldmath\bfseries\raggedright}

% Chapter Headers
\chapterstyle{verville}

% Page Headers / Footers
\copypagestyle{myruled}{ruled} % Draw formatting from existing 'ruled' style
\makeoddhead{myruled}{}{}{\scshape\subject}
\makeevenfoot{myruled}{}{\thepage}{}
\makeoddfoot{myruled}{}{\thepage}{}
\pagestyle{myruled}
\setfootins{0.5cm}{0.5cm}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

% Titlepage
\title{\subject}
\author{Alex Rutar\thanks{\itshape arutar@uwaterloo.ca}\\ University of Waterloo}
\date{\semester\thanks{Last updated: \today}}

\begin{document}
\pagenumbering{gobble}
\hypersetup{pageanchor=false}
\maketitle
\newpage
\frontmatter
\hypersetup{pageanchor=true}
\tableofcontents*
\newpage
\mainmatter


% main document ----------------------------------------------------------------
\chapter{Stochastic Calculus}
\begin{definition}
    Given a measure space $(\Omega,\mathcal{F},\pr)$, a measurable function $f:\Omega\to\R$ is called a \defn{random variable}.
\end{definition}
\begin{definition}
    A \defn{stochastic process} $X=\{X_t\}_{t\in T}$ is a collection of random variables defined on some probability space $(\Omega,\mathcal{F},\pr)$.
\end{definition}
Typically $t\in\Z^+$ or $t\in\R^+$ (including 0); $t$ is a discrete or continuous time parameter.
Given some $\omega\in\Omega$ the map $t\mapsto X_t(\omega)$ is called a \defn{realization} or \defn{path} of this process.
We will regard $\{X_t\}_{t\geq 0}$ as a random element in some path space, equipped with a proper $\sigma-$algebra and probability.

Consider $X_t(\omega)$ as a function $X:[0,\infty)\times\Omega\to \R$ equipped with the product $\sigma-$algebra.

\begin{definition}
    The \defn{distribution} of a stochastic process is the collection of all its finite-dimensional distributions.
\end{definition}
Two processes $X$ and $Y$ can be ``the same'' in different senses:
\begin{definition}
    Two process $X=\{X_t\}_{t\geq 0}$ and $Y=\{Y_t\}_{t\geq 0}$ are called \defn{distinguishable} if almost all their sample paths agree; in other words, $\pr(X_t=Y_t,0\leq t<\infty)=1$.
    We say that $Y$ is a \defn{modification} of $X$ if for each $t\geq 0$ we have $\pr(X_t=Y_t)=1$.
    Finally, $X$ and $Y$ are said to have the \defn{same distribution} if all the finite dimensional distributions agree.
    In other words, if for all $n\in\N$ and $0\leq t_1<t_2<\cdots<t_n<\infty$, we have $(X_{t_1},\ldots,X_{t_n})\overset{d}{=}(Y_{t_1},\ldots,Y_{t_n})$.
\end{definition}
\begin{example}
    Let $X$ be a continuous stochastic process and $N$ a Poisson point process on $[0,\infty)$.
    Then define
    \begin{equation*}
        Y_t :=
        \begin{cases}
            X_t &: t\notin N\\
            X_t+1 &:t\in N
        \end{cases}
    \end{equation*}
    Thus $\pr(X_t=Y_t)=1$ for all $t$, so $X$ is a modification of $Y$.
    However, $\pr(X_t=Y_t,t\geq 0)=0$, so that $X$ and $Y$ are not indistinguishable.
\end{example}
A filtration formalizes the idea of ``information acquired over time''.
\begin{definition}
    Let $(\Omega,\mathcal{F},\pr)$ be a probability space.
    A \defn{filtration} is a non-decreasing family $\{\mathcal{F}_t\}_{t\geq 0}$ of sub-$\sigma$-algebras of $\mathcal{F}$ so that $\mathcal{F}_s\subseteq\mathcal{F}_t\subseteq\mathcal{F}$ for $0\leq s<t<\infty$.
    We write $F_\infty=\sigma(\bigcup_{t\geq 0}\mathcal{F}_t)$.
\end{definition}
Let $\{X_t\}_{t\geq 0}$ be a stochastic process.
The filtration generated by $\{X_t\}_{t\geq 0}$ is $\{\sigma(X_s:0\leq s\leq t)\}_{t\geq 0}$, in other words $\mathcal{F}_t$ is the smallest $\sigma-$algbra which makes $X_s$ measurable for all $s\in[0,t]$.
\begin{definition}
    A stochastic process $\{X_t\}_{t\geq 0}$ is called \defn{adapted} to a filtration $\{\mathcal{F}_t\}_{t\geq 0}$ if $X_t$ is $\mathcal{F}_t-$measurable for every $t\geq 0$.
\end{definition}
The filtration generated by $\{X_t\}_{t\geq 0}$ is the smallest filtration which makes $(X_t)_{t\geq 0}$ adapted.

A filtration $\{\mathcal{F}_t\}_{t\geq 0}$ is said to satisfy the ``usual condition'' if
\begin{enumerate}[nl]
    \item It is right-continuous: $\lim_{s\to t^+}:=\bigcap_{s>t}\mathcal{F}_s=\mathcal{F}_t$
    \item $\mathcal{F}_0$ contains all the $\pr-$null events in $\mathcal{F}$.
\end{enumerate}
\section{Martingale Theory}
Consider a filtered space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in S})$ where $S=\N$ or $S=\R^+$.
\begin{definition}
    A \defn{random time} $T$ is called a stopping time if $\{T\leq t\}\in\mathcal{F}_t$ (``we know it happens when it happens'').
\end{definition}
\begin{example}
    \begin{enumerate}[nl,r]
        \item Constants are trivial stopping times.
        \item Last hit a constant before $N$ is not a stopping time
    \end{enumerate}
\end{example}
\begin{proposition}
    If $S,T$ are stopping times, $T\vee S$, $T\wedge S$, $T+S$ are stopping times.
\end{proposition}
\begin{proof}
    That $T\wedge S$ and $T\vee S$ are stopping times are trivial.
    For $T+S$, $\{T+S>t\}=\{T=0,S>t\}\cup\{0<T\leq t,T+S>t\}\cup\{T>t\}$.
    It suffices to prove that
    \begin{equation*}
        \{0<T\leq t<T+S>t\}=\bigcup_{\substack{r\in\Q^+\\0<r<t}}\{r<T\leq t,S>t-r\}.
    \end{equation*}
    If there exists $r$ with $r<T\leq t$, then $S>t=r$ and $S+T>r+(t-r)=t$, so $\supseteq$ holds.
    Conversely, if $0<T\leq t$ and $T+S\geq t$< then there exists $r\in\Q$ such that $r<T$ and $r+S>t$.
    Hence $r<T\leq t$ and $S>t-r$.
\end{proof}
\begin{definition}
    The $\sigma-$algebra generated by a stopping time $T$ is the collection of all the events $A$ for which $A\cap\{T\leq t\}\in\mathcal{F}_t$ for every $t\geq 0$.
    This is the ``information you collect until the stopping time''.
\end{definition}
Exercise: show that the collection given in the definition above is actually a $\sigma-$algebra.

We write $X_{T\wedge t}$ is a random variable evaluated at time $T\wedge t$ (or $T$); in other words, $(X_{T\wedge t})(\omega)=X_{T\wedge t}(\omega)$.
Then $\{T_{T\wedge t}\}_{t\geq 0}$, or $X^T$, is a stochastic process stopped at time $t$.
\begin{definition}
    Consider a filtered probability space $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$.
    A $\{\mathcal{F}_t\}_{t\in T}-$adaped proces $\{X_t\}_{t\in T}$ is said to be a \defn{submartingale} if
    \begin{enumerate}[nl,r]
        \item For all $t\in T$, $X_t\in L^1(\Omega,\mathcal{F},\ps)$
        \item For all $s<t$ where $s,t\in T$,
            \begin{equation*}
                \E(X_t | \mathcal{F}_s)\geq X_s\qquad \ps\text{ a.s.}
            \end{equation*}
    \end{enumerate}
    and it is a \defn{supermartingale} with the inequality in (ii) reversed.
    Then $\{X_t\}_{t\in T}$ is a \defn{martingale} if it is both a submartingale and supermartingale.
\end{definition}
Clearly if $0\leq s<t$, then $\E(X_0)\leq \E(X_s)\leq \E(X_t)$ in the submartingale case, and other appropriate statements in the other case.
One of the goals of martingale theory is to extend these results with respect to stopping times, rather than with respect to fixed time.

Let $X=\{X_n\}_{n=0}^\infty$ be a discrete time process, and fix levels $a<b$.
The number $U_N[a,b]$ of \defn{upcrossings} of $[a,b]$ by $X$ by time $N$ is the largest $k\in\Z^+$ such that there exists times $a\leq s_1<t_1<s_2<t_2<\cdots<s_k<t_k\leq N$ such that for all $i$, $s_i\leq a$ and $t_i\geq b$.
\begin{definition}
    A process $X=\{X_n\}_{n=0}^\infty$ is called \defn{previsible} if $X_n\in\mathcal{F}_{n-1}$ for all $n\geq 1$.
\end{definition}
Let $C=\{C_n\}_{n=1}^\infty$ be a previsible process.
Then the martingale transform of $X$ by $C$, denoted by $C\cdot X$, is defined as $(C\cdot X)_n=\sum_{k=1}^n C_k(X_k-X_{k-1})$ for $n>0$ and $(C\cdot X)_0=0$.
\begin{proposition}
    \begin{enumerate}[nl]
        \item Let $C$ be a bounded, non-negative previsible process and $X$ a (super)matingale.
            Then $C\cdot X$ is a (super)martingale which is null at 0.
        \item Let $C$ be a bounded, previsible process and $X$ a martingale.
            Then $C\cdot X$ is a martingale which is null at 0.
        \item If $C$ is an $L^2$ previsible process (resp. non-negative) and $X$ an $L^2$ martingale (resp. supermartingale), then $C\cdot X$ is a martingale (resp. supermartingale).
\end{proposition}
\begin{proof}
    \begin{enumerate}[nl]
        \item We have
            \begin{align*}
                \E[(C\cdot X)_n-(C\cdot X)_{n-1}|\mathcal{F}_{n-1}] &= \E[C_n(X_n-X_{n-1})|\mathcal{F}_{n-1}]\\
                                                                    &= C_n\cdot \E[X_n-X_{n-1}|\mathcal{F}_{n-1}]\\
                                                                    &\leq 0
            \end{align*}
            where the second line follows since $C$ is previsible.
            Since $C$ is bounded, $C\cdot X$ is integrable.
        \item Consider $C+k$ where $k$ is a constant and $k\geq|C_n(w)|$ for all $n$ and $w$.
        \item Similar, but the integrability is now guaranteed by HÃ¶lder's inequality.
    \end{enumerate}
\end{proof}
We now have the following result:
\begin{proposition}[Doob's Upcrossing Inequality]
    Let $X$ be a supermartingale.
    Then $(b-a)\E(U_N[a,b])\leq\E((X_N-a)^-)$.
\end{proposition}
\begin{proof}
    Define a process $\{C_n\}_{n=1}^\infty$ by
    \begin{align*}
        C_1&:=\chi_{\{X_0<a\}}\\
        C_k&:=\chi_{\{C_{n-1}=1\}}\chi_{\{X_{n-1}\leq b\}}+\chi_{\{C_{n-1}=0\}}\chi_{\{X_{n-1}<a\}}.
    \end{align*}
    Let $Y=C\cdot X$, i.e. $Y_n=\sum_{k=1}^n C_k(X_k-X_{k-1})$ almost surely.
    Note that each finished upcrossing increases the value of $Y$ by at least $b-a$, we have
    \begin{equation*}
        Y_N\geq(b-a)U_N[a,b]-(X_n-a)^-
    \end{equation*}
    where $(X_N-a)^-$ is the upper bound for the ``loss'' due to the last unfinished upcrossing.
    Since $C$ is non-negative, bounded, and previsible, $Y$ is a supermartingale.
    Moreover, $\E(Y_1)\leq 0$, so $\E(Y_N)\leq\E(Y_1)\leq 0$.
    Thus
    \begin{equation*}
        (b-a)\E U_[a,b]\leq\E[(X_n-a)^i]
    \end{equation*}
    as required.
\end{proof}
\begin{theorem}
    Suppose $\{X_t\}_{t\in T}$ is a martingale with $\E(X_t^2)\leq B<\infty$ for any $t\in T$ and $B$ not depending on $t$.
    Then $X_t$ converge in $L^2$ to a limit $X_\infty$.
\end{theorem}
\begin{proof}
    Note that we have the result for both discrete and continuous time martingales.
    By discretization, it is clear that it suffices to rove the result for discrete case.

    We have the following orthogonality between the increments of a martingale $\{X_n\}_{n=0}^\infty$: if $n_1<n_2\leq n_2<n_4$, then
    \begin{equation*}
        \E[(X_{n_2}-X_{n_1})(X_{n_4}-X_{n_3})]=0.
    \end{equation*}
    This result follows by conditioning on $\mathcal{F}_{n_3}$ and applying the law of total expectation.

    Now the proof proceeds.
    Set $Y_n:=X_n-X_{n-1}$, so
    \begin{equation*}
        \norm{X_n}_2^2=\E(X_n^2)=\sum_{i=1}^n\norm{Y_i}_2^2
    \end{equation*}
    and $\sum_{i=0}^n\norm{Y_n}_2^2\leq B$ for all $n$, so that $\sum_{i=0}^\infty\norm{Y_n}_2^2\leq B$.
    Thus $\{X_n\}_{n=0}^\infty$ is Cauchy in $L^2(\Omega,\mathcal{F},\pr)$.
\end{proof}
\begin{theorem}[Optimal Sampling for Bounded Stopping Times in Discrete Times]
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ supermartingale and $S,T$ be $\{\mathcal{F}_n\}-$stopping times such that $0\leq S\leq T\leq N$ for some constant $N<\infty$.
    Then $X_T$ is integrable and $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely.
\end{theorem}
\begin{proof}
    Notice that
    \begin{equation*}
        |X_T|\leq|X_0|+|X_1|+\cdots+|X_N|
    \end{equation*}
    so that $\E(|X_T|)<\infty$.
    To prove that $\E(X_t|\mathcal{F}_s)\leq X_s$ a.s., it suffices to prove that $\E(X_T;A):=\int_A X_Td\ps\leq\int_A X_s d\ps=:\E(X_s;A)$ for all $A\in\mathcal{F}_s$.
    Assuming this, then
    \begin{equation*}
        \E(\E(X_T|\mathcal{F}_s)-X_s;A)\leq 0
    \end{equation*}
    for all $A\in\mathcal{F}_s$, so we may take $A=A_0:=\{\E(X_T|\mathcal{F}_s)-X_s>0\}$, so that $\pr(A_0)=0$.

    Let's prove the required statement.
    First note that
    \begin{equation*}
        \sum_{n=1}^N \chi_{\{S<n\leq T\}}(X_n-X_{n-1})=X_T-X_S
    \end{equation*}
    and taking expectation over $A$ on both sides
    \begin{align*}
        \E(X_T-X_S;A) &= \sum_{n=1}^N\E(\chi_{\{S<n\leq T\}}(X_n-X_{n-1});A)\\
                      &= \sum_{n=1}^N\E(X_n-X_{n-1};A\cap\{S<n\leq T\})
    \end{align*}
    But $A\cap\{S<n\leq T\}=A\cap\{S\leq n-1\}\cap\{n-1<T\}\in\mathcal{F}_{n-1}$.
    Thus
    \begin{equation*}
        \E(X_n-X_{n-1}; A\cap\{S<n\leq T\})=\E(\E(X_n-X_{n-1}|\mathcal{F}_{n-1});A\cap\{S<n\leq T\})\leq 0
    \end{equation*}
    so the required statement holds.
\end{proof}
\begin{definition}
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ a supermartingale.
    We say that $\{X_n\}_{n=0}^\infty$ is \defn{closed} by a random variable $X_\infty$ if $X_\infty$ is $\mathcal{F}_\infty-$measurable and $X_n\geq\E(X_\infty|\mathcal{F}_n)$ almost surely for all $n=0,1,\ldots$.
\end{definition}
Similar statements hold with $X_n\leq\E(X_\infty|\mathcal{F}_n)$ for a submartingale, or equality with a martingale.
\begin{proposition}
    Suppose that $\{X_n\}_{n=0}^\infty$ is a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ a non-negative supermartingale, and $X_\infty=0$.
    If $S,T:\Omega\to\overline{\Z^+}$ are $\{\mathcal{F}_n\}-$stopping times, $S\leq T$, then
    \begin{enumerate}[nl]
        \item $\E(X_T)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[nl]
        \item Note that $X_T\leq\liminf_{n\to\infty}X_{T\wedge n}$ where $T\wedge n$ and $0$ are two bounded stopping times.
            Thus $\E(X_{T\wedge n})\leq\E(X_0)$ for all $n=0,1,\ldots$.
            Thus by Fatou's lemma
            \begin{align*}
                \E(X_T)&\leq\E(\liminf_{n\to\infty} X_{T\wedge n})\leq\liminf_{n\to\infty}\E(X_{T\wedge n})\\
                       &\leq\E(X_0)<\infty.
            \end{align*}
        \item Let $A\in\mathcal{F}_s$.
            For $n=0,1,\ldots,$,
            \begin{align*}
                \E(X_T;A\cap\{T\leq n\}) &= \E(X_{T\wedge n};A\cap\{T\leq n\})\\
                                         &\leq \E(X_{T\wedge n};A\cap\{S\leq n\})\\
                                         &\leq \E(X_{S\wedge n}; A\cap\{S\leq n\})
            \end{align*}
            Note that $S\wedge n$ and $T\wedge n$ are two bounded stopping times with $S\wedge n\leq T\wedge n$.
            Also, $A\cap\{S\leq n\}\in\mathcal{F}_{S\wedge n}$.
            Then apply the optimal sampling theorem for bounded stopping times.
            By the monotone convergence theorem,
            \begin{equation*}
                \lim_{n\to\infty}\E(X_T;A\cap\{T\leq n\})
            \end{equation*}
            and similarly for $S$.
            Thus
            \begin{equation*}
                \E(X_T;A\cap\{T<\infty\})\leq\E(X_S:A\cap\{S<\infty\})
            \end{equation*}
            so that
            \begin{equation*}
                \E(X_T;A\cap\{T=\infty\})=\E(X_S:A\cap\{S=\infty\})=0
            \end{equation*}
            and $\E(X_T;A)\leq\E(X_S;A)$.
            Since this holds for all $A\in\mathcal{F}_s$, $\E(X_T:\mathcal{F}_s)\leq X_s$.
    \end{enumerate}
\end{proof}
\begin{lemma}
    Let $\{M_n\}_{n=0}^\infty$ be a martingale closed by $M_\infty$.
    Let $T:\Omega\to\overline{\Z^+}$ be a stopping time.
    Then $M_T=\E(M_\infty|\mathcal{F}_T)$.
\end{lemma}
\begin{proof}
    First assume $M_\infty\geq 0$.
    For any $A\in\mathcal{F}_T$,
    \begin{align*}
        E(M_T;A)&=\sum_{n=0}^\infty\E(M_n;A\cap\{T=n\})\\
                &=\sum_{n=0}^\infty\E(M_\infty,A\cap\{T=n\})\\
                &= \E(M_\infty;A).
    \end{align*}
    For the general case, decompose $M_\infty$ into positive and negative parts.
\end{proof}
\begin{theorem}[Optimal Sampling for Closed Supermartingales in Discrete Time]
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ supermartingale closed by $X_\infty$.
    Let $S,T:\Omega\to\overline{\Z^+}$ be two $\{\mathcal{F}_n\}_{n=0}^\infty$ stopping times with $S\leq T$.
    Then
    \begin{enumerate}[nl]
        \item $\E(|X_T|)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[nl]
        \item Define $M_n=\E(X_\infty|\mathcal{F}_n)$ and $A_n=X_n-M_n$ for $n=0,1,\ldots,\infty$.
            Since $\{X_n\}_{n=0}^\infty$ is a supermartingale closed by $X_\infty$, $A_n\geq 0$, $A_\infty=0$, and for $m\leq n$,
            \begin{align*}
                \E(A_n|\mathcal{F}_m)&=\E(X_n-\E(X_\infty|\mathcal{F}_n)|\mathcal{F}_m)\\
                                     &\leq X_m-\E(X_\infty|\mathcal{F}_m)\\
                                     &= A_m
            \end{align*}
            Thus $\{A_n\}_{n=0}^\infty$ is a non-negative supermartingale with $A_\infty=0$.

            One can prove that $\{M_n\}_{n=0}^\infty$ is a uniformly integrable martingale, i.e. $\lim_{c\to\infty}\sup_{n\in\N}\E(|X_n|;|X_n|>c)=0$.

            By the previous proposition, $\E(A_T)<\infty$.
            On the other hand, $\E(|M_T|)<\infty$ by definition of the $\{M_n\}$.
            Thus $\E(|X_T|)<\infty$.
        \item Apply the previous lemma so $\E(M_T|\mathcal{F}_s)=\E(\E(M_\infty|\mathcal{F}_T)\mathcal{F}_S)=\E(M_\infty|\mathcal{F}_s)=M_s$ by the optimal sampling theorem for closed martingales.
            Meanwhile, as $\{A_n\}_{n=0}^\infty$ is a non-negative supermartingale with $A_\infty=0$, $\E(A_T|\mathcal{F}_s)\leq A_s$ almost surely.
            Thus $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely.
    \end{enumerate}
\end{proof}
To prepare for the optimal sampling theorem for closed supermartingales in continuous time, we introduce one more discrete time notion:
\begin{definition}
    A \defn{negatively indexed supermartingale} $\{X_n\}_{n=0,-1,\ldots}$ and $\{\mathcal{F}_n\}_{n=0,-1,\ldots}$ where $\mathcal{F}_m\subseteq\mathcal{F}_n$ for $m\leq n$.
    Then $X_n\in L^1(\Omega,\mathcal{F},\pr)$, $\{X_n\}$ is adapted and $\E(X_n|\mathcal{F}_m)\leq X_m$.
\end{definition}
\begin{theorem}
    Let $\{X_n\}_{n=0,-1,\ldots}$ be a negatively indexed supermartingale such that $\sup_{-\infty<n\leq 0}\E(X_n)<\infty$.
    Then $\{X_n\}_{n=0,-1,\ldots}$ is uniformly integrable.
\end{theorem}
\end{document}
