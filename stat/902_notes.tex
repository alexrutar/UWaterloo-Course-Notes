
% header -----------------------------------------------------------------------
% Template created by texnew (author: Alex Rutar); info can be found at 'https://github.com/alexrutar/texnew'.
% version (1.13)


% doctype ----------------------------------------------------------------------
\documentclass[11pt, a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
\usepackage[protrusion=true,expansion=true]{microtype}


% packages ---------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{etoolbox}
\usepackage{braket}

% Set enimitem
\usepackage{enumitem}
\SetEnumitemKey{nl}{nolistsep}
\SetEnumitemKey{r}{label=(\roman*)}

% Set tikz
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.15}
\usetikzlibrary{intersections,positioning,cd}
\usetikzlibrary{arrows,arrows.meta}
\tikzcdset{arrow style=tikz,diagrams={>=stealth}}

% Set hyperref
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\newcommand\myshade{85}
\colorlet{mylinkcolor}{violet}
\colorlet{mycitecolor}{orange!50!yellow}
\colorlet{myurlcolor}{green!50!blue}

\hypersetup{
  linkcolor  = mylinkcolor!\myshade!black,
  citecolor  = mycitecolor!\myshade!black,
  urlcolor   = myurlcolor!\myshade!black,
  colorlinks = true,
}


% macros -----------------------------------------------------------------------
\DeclareMathOperator{\N}{{\mathbb{N}}}
\DeclareMathOperator{\Q}{{\mathbb{Q}}}
\DeclareMathOperator{\Z}{{\mathbb{Z}}}
\DeclareMathOperator{\R}{{\mathbb{R}}}
\DeclareMathOperator{\C}{{\mathbb{C}}}
\DeclareMathOperator{\F}{{\mathbb{F}}}

% Boldface includes math
\newcommand{\mbf}[1]{{\boldmath\bfseries #1}}

% proof implications
\newcommand{\imp}[2]{($#1\Rightarrow#2$)\hspace{0.2cm}}
\newcommand{\impe}[2]{($#1\Leftrightarrow#2$)\hspace{0.2cm}}
\newcommand{\impr}{{($\Longrightarrow$)\hspace{0.2cm}}}
\newcommand{\impl}{{($\Longleftarrow$)\hspace{0.2cm}}}

% align macros
\newcommand{\agspace}{\ensuremath{\phantom{--}}}
\newcommand{\agvdots}{\ensuremath{\hspace{0.16cm}\vdots}}

% convenient brackets
\newcommand{\brac}[1]{\ensuremath{\left\langle #1 \right\rangle}}
\newcommand{\norm}[1]{\ensuremath{\left\lVert#1\right\rVert}}
\newcommand{\abs}[1]{\ensuremath{\left\lvert#1\right\rvert}}

% arrows
\newcommand{\lto}[0]{\ensuremath{\longrightarrow}}
\newcommand{\fto}[1]{\ensuremath{\xrightarrow{\scriptstyle{#1}}}}
\newcommand{\hto}[0]{\ensuremath{\hookrightarrow}}
\newcommand{\mapsfrom}[0]{\mathrel{\reflectbox{\ensuremath{\mapsto}}}}
 
% Divides, Not Divides
\renewcommand{\div}{\bigm|}
\newcommand{\ndiv}{%
    \mathrel{\mkern.5mu % small adjustment
        % superimpose \nmid to \big|
        \ooalign{\hidewidth$\big|$\hidewidth\cr$/$\cr}%
    }%
}

% Convenient overline
\newcommand{\ol}[1]{\ensuremath{\overline{#1}}}

% Big \cdot
\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

% Big and small Disjoint union
\makeatletter
\providecommand*{\cupdot}{%
  \mathbin{%
    \mathpalette\@cupdot{}%
  }%
}
\newcommand*{\@cupdot}[2]{%
  \ooalign{%
    $\m@th#1\cup$\cr
    \sbox0{$#1\cup$}%
    \dimen@=\ht0 %
    \sbox0{$\m@th#1\cdot$}%
    \advance\dimen@ by -\ht0 %
    \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }%
}

\providecommand*{\bigcupdot}{%
  \mathop{%
    \vphantom{\bigcup}%
    \mathpalette\@bigcupdot{}%
  }%
}
\newcommand*{\@bigcupdot}[2]{%
  \ooalign{%
    $\m@th#1\bigcup$\cr
    \sbox0{$#1\bigcup$}%
    \dimen@=\ht0 %
    \advance\dimen@ by -\dp0 %
    \sbox0{\scalebox{2}{$\m@th#1\cdot$}}%
    \advance\dimen@ by -\ht0 %
    \dimen@=.5\dimen@
    \hidewidth\raise\dimen@\box0\hidewidth
  }%
}
\makeatother


% macros (theorem) -------------------------------------------------------------
\usepackage[thmmarks,amsmath,hyperref]{ntheorem}
\usepackage[capitalise,nameinlink]{cleveref}

% Numbered Statements
\theoremstyle{change}
\theoremindent\parindent
\theorembodyfont{\itshape}
\theoremheaderfont{\bfseries\boldmath}
\newtheorem{theorem}{Theorem.}[section]
\newtheorem{lemma}[theorem]{Lemma.}
\newtheorem{corollary}[theorem]{Corollary.}
\newtheorem{proposition}[theorem]{Proposition.}

% Claim environment
\theoremstyle{plain}
\theorempreskip{0.2cm}
\theorempostskip{0.2cm}
\theoremheaderfont{\scshape}
\newtheorem{claim}{Claim}
\renewcommand\theclaim{\Roman{claim}}
\AtBeginEnvironment{theorem}{\setcounter{claim}{0}}

% Un-numbered Statements
\theorempreskip{0.1cm}
\theorempostskip{0.1cm}
\theoremindent0.0cm
\theoremstyle{nonumberplain}
\theorembodyfont{\upshape}
\theoremheaderfont{\bfseries\itshape}
\newtheorem{definition}{Definition.}
\theoremheaderfont{\itshape}
\newtheorem{example}{Example.}
\newtheorem{remark}{Remark.}

% Proof / solution environments
\theoremseparator{}
\theoremheaderfont{\hspace*{\parindent}\scshape}
\theoremsymbol{$//$}
\newtheorem{solution}{Sol'n}
\theoremsymbol{$\blacksquare$}
\theorempostskip{0.4cm}
\newtheorem{proof}{Proof}
\theoremsymbol{}
\newtheorem{nmproof}{Proof}

% Format references
\crefformat{equation}{(#2#1#3)}
\Crefformat{theorem}{#2Thm. #1#3}
\Crefformat{lemma}{#2Lem. #1#3}
\Crefformat{proposition}{#2Prop. #1#3}
\Crefformat{corollary}{#2Cor. #1#3}
\crefformat{theorem}{#2Theorem #1#3}
\crefformat{lemma}{#2Lemma #1#3}
\crefformat{proposition}{#2Proposition #1#3}
\crefformat{corollary}{#2Corollary #1#3}


% macros (algebra) -------------------------------------------------------------
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\chr}{char}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Nil}{Nil}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Tor}{Tor}

% Lagrange symbol
\newcommand{\lgs}[2]{\ensuremath{\left(\frac{#1}{#2}\right)}}

% Quotient (larger in display mode)
\newcommand{\quot}[2]{\mathchoice{\left.\raisebox{0.14em}{$#1$}\middle/\raisebox{-0.14em}{$#2$}\right.}
                                 {\left.\raisebox{0.08em}{$#1$}\middle/\raisebox{-0.08em}{$#2$}\right.}
                                 {\left.\raisebox{0.03em}{$#1$}\middle/\raisebox{-0.03em}{$#2$}\right.}
                                 {\left.\raisebox{0em}{$#1$}\middle/\raisebox{0em}{$#2$}\right.}}


% macros (analysis) ------------------------------------------------------------
\DeclareMathOperator{\M}{{\mathcal{M}}}
\DeclareMathOperator{\B}{{\mathcal{B}}}
\DeclareMathOperator{\ps}{{\mathcal{P}}}
\DeclareMathOperator{\pr}{{\mathbb{P}}}
\DeclareMathOperator{\E}{{\mathbb{E}}}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\sgn}{sgn}

\renewcommand{\Re}{\ensuremath{\operatorname{Re}}}
\renewcommand{\Im}{\ensuremath{\operatorname{Im}}}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}


% file-specific preamble -------------------------------------------------------
\newcommand{\defname}[1]{{\textit{(#1)}:}}
\newcommand{\exname}[1]{{\textit{#1}:}}
\newcommand{\defn}[1]{{\boldmath\bfseries #1}}
% \usepackage{therefore}
\newcommand{\TODO}[1]{[\textit{\textbf{TODO: #1}}]}
\newcommand{\NOTE}[1]{[\textit{\textbf{NOTE: #1}}]}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator{\ext}{ext}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\Pol}{Pol}
\newcommand{\bdim}{\ensuremath{\dim_B}}
\newcommand{\ubdim}{\ensuremath{\overline{\dim}_B}}
\newcommand{\lbdim}{\ensuremath{\underline{\dim}_B}}
\newcommand{\cwx}{\ensuremath{\overline{\operatorname{conv}}^{w^*}\,}}
\newcommand{\idc}{\mathbf{1}}
\newcommand{\FA}{\ensuremath{\operatorname{F}\!\operatorname{A}}}
\newcommand{\cw}{\ensuremath{\overline{\operatorname{conv}}\,}}

% Tons of notation:
% \newcommand{\Lip}[1]{\ensuremath{\operatorname{Lip}_{\F}(#1)}}
\newcommand{\Lipspace}{\ensuremath{\operatorname{Lip}_{\F}(X,d)}}


\newcommand{\lp}[1]{\ensuremath{\ell^{#1}}}
\newcommand{\csn}{\ensuremath{\mathbf{c}}}
\newcommand{\csz}{\ensuremath{\mathbf{c}_0}}
\newcommand{\lpspace}[1]{\ensuremath{\ell^{#1}_{\F}}}
\newcommand{\Lp}[1]{\ensuremath{L^{#1}_{\F}}}
% \newcommand{\Lpm}{\ensuremath{L^{#1}_{\F}(X,\mathcal{M},\mu)}}
\DeclareMathOperator{\Lip}{Lip}
\newcommand{\lbr}[1]{\ensuremath{\left[#1\right]}}
\newcommand{\inr}[1]{\ensuremath{\left(#1\right)}}


% constants --------------------------------------------------------------------
\newcommand{\subject}{Martingales and Stochastic Calculus}
\newcommand{\semester}{Winter 2020}


% formatting -------------------------------------------------------------------
% Fonts
\usepackage{kpfonts}
\usepackage{dsfont}

% Adjust numbering
\numberwithin{equation}{section}
\counterwithin{figure}{section}
\counterwithout{section}{chapter}
\counterwithin*{chapter}{part}

% Footnote
\setfootins{0.5cm}{0.5cm} % footer space above
\renewcommand*{\thefootnote}{\fnsymbol{footnote}} % footnote symbol

% Table of Contents
\renewcommand{\thechapter}{\Roman{chapter}}
\renewcommand*{\cftchaptername}{Chapter } % Place 'Chapter' before roman
\setlength\cftchapternumwidth{4em} % Add space before chapter name
\cftpagenumbersoff{chapter} % Turn off page numbers for chapter
\maxtocdepth{subsection} % table of contents up to section

% Section / Subsection headers
\setsecnumdepth{subsection} % numbering up to and including "subsection"
\newcommand*{\shortcenter}[1]{%
    \sethangfrom{\noindent ##1}%
    \Large\boldmath\scshape\bfseries
    \centering
\parbox{5in}{\centering #1}\par}
\setsecheadstyle{\shortcenter}
\setsubsecheadstyle{\large\scshape\boldmath\bfseries\raggedright}

% Chapter Headers
\chapterstyle{verville}

% Page Headers / Footers
\copypagestyle{myruled}{ruled} % Draw formatting from existing 'ruled' style
\makeoddhead{myruled}{}{}{\scshape\subject}
\makeevenfoot{myruled}{}{\thepage}{}
\makeoddfoot{myruled}{}{\thepage}{}
\pagestyle{myruled}
\setfootins{0.5cm}{0.5cm}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

% Titlepage
\title{\subject}
\author{Alex Rutar\thanks{\itshape arutar@uwaterloo.ca}\\ University of Waterloo}
\date{\semester\thanks{Last updated: \today}}

\begin{document}
\pagenumbering{gobble}
\hypersetup{pageanchor=false}
\maketitle
\newpage
\frontmatter
\hypersetup{pageanchor=true}
\tableofcontents*
\newpage
\mainmatter


% main document ----------------------------------------------------------------
\chapter{Stochastic Calculus}
\section{Measure Theory for Probability}
\begin{definition}
    Given a measure space $(\Omega,\mathcal{F},\pr)$, a measurable function $X:\Omega\to\R$ is called a \defn{random variable}.
    If $X$ is a random variable, then we define the \defn{distribution} of $X$ to be the measure Borel measure $\mu$ on $\R$ given by $\mu(E)=\pr(X^{-1}(E))$.
\end{definition}
\subsection{Conditional Expectation}
\begin{theorem}[Kolmogorov]
    Suppose $X\in L^1(\Omega,\mathcal{F},\pr)$ and $\mathcal{G}\subset\mathcal{F}$ is a sub-$\sigma$-algebra.
    Then there exists some $Z\in L^1(\Omega,\mathcal{G},\pr)$ such that for each $A\in\mathcal{G}$
    \begin{equation*}
        \int_A Xd\pr=\int_A Zd\pr.
    \end{equation*}
    Moreover, if $\tilde Z\in L^1(\Omega,\mathcal{G},\pr)$ satisfies the above constraint, then $\pr\{x\in\Omega:\tilde Z(x)=Z(x)\}=1$.
\end{theorem}
\begin{definition}
    In the context of the above theorem, we write $Z=\E[X|\mathcal{G}]$ and call $\E[X|\mathcal{G}]$ a \defn{conditional expectation} with respect to $\mathcal{G}$.
    Certainly $\E[X|\mathcal{G}]$ nee not be pointwise unique.
\end{definition}
\begin{theorem}[Properties of Conditional Expectation]
    Suppose $X,Y$ are random variables on $(\Omega,\mathcal{F},\pr)$ with $X\in L^1(\Omega,\mathcal{F},\pr)$.
    Let $\mathcal{G}\subset\mathcal{F}$ be a sub-$\sigma$-algebra.
    Then
    \begin{enumerate}[nl]
        \item $\E[X|\mathcal{G}]\geq 0$ a.s. whenever $X\geq 0$ a.s.
        \item For $\alpha,\beta\in\R$ and $Y\in L^1(\Omega,\mathcal{F},\pr)$, $\E[\alpha X+\beta Y|\mathcal{G}]=\alpha\E[X|\mathcal{G}]+\beta\E[Y|\mathcal{G}]$ a.s.
        \item If $XY\in L^1(\Omega,\mathcal{F},\pr)$ and $Y$ is $\mathcal{G}-$measurable, then $\E[XY|\mathcal{G}]=Y\E[X|\mathcal{G}]$
        \item If $\mathcal{H}\subset\mathcal{G}$ is a $\sigma$-algebra, then $\E[\E[X|\mathcal{G}]|\mathcal{H}]=\E[X|\mathcal{H}]$ a.s.
        \item $\E[\E[X|\mathcal{G}]]=\E[X]$
        \item If $\mathcal{H}$ is a $\sigma$-algebra which is independent of the $\sigma-$algebra $\sigma\{\sigma\{X\},\mathcal{G}\}$, then $\E[X|\sigma\{\mathcal{G},\mathcal{H}\}]=\E[X|\mathcal{G}]$.
            In particular, $\E[X|\mathcal{G}]=\E[X]$ a.s. whenever $\sigma\{X\}$ and $\mathcal{H}$ are independent.
    \end{enumerate}
\end{theorem}
\subsection{Stochastic Processes}
\begin{definition}
    A \defn{stochastic process} $X=\{X_t\}_{t\in \mathcal{T}}$ is a collection of random variables defined on some probability space $(\Omega,\mathcal{F},\pr)$.
\end{definition}
Typically, we assume that $\mathcal{T}=\Z_{\geq 0}$ or $\mathcal{T}=\R_{\geq 0}$ and equip $\mathcal{T}$ with the order topology
Intuitively, we expect $t$ to be a discrete or continuous time parameter.
Given some $\omega\in\Omega$, the map $t\mapsto X_t(\omega)$ is called a \defn{realization} or \defn{path} of this process.
One of the goals of this section is to treat $\{X_t\}_{t\geq 0}$ as a random element in some path space, equipped with a proper $\sigma-$algebra and probability.

\begin{definition}
    We say that a stochastic process is \defn{$L^p$} if $X_t$ is $L^p$ for each $t\in\mathcal{T}$.
    Then we say that a stochastic process is \defn{uniformly $L^p$} if there exists some $B\in\R$ so that $\int_\Omega|X_t|^pd\pr\leq B$ for each $t\in\mathcal{T}$.
\end{definition}
Consider $X_t(\omega)$ as a function $X:\mathcal{T}\times\Omega\to \R$ equipped with the product $\sigma-$algebra.
\begin{definition}
    The \defn{distribution} of a stochastic process is the collection of all its finite-dimensional distributions, i.e. the collection of all the distributions of $\{X_{t_1},\ldots,X_{t_k}\}$ for and $k\in\N$ and $t_i\in \mathcal{T}$.
\end{definition}
There are a number of ways to say that two processes $X$ and $Y$ are equivalent, which we organize in decreasing order of strength.
\begin{definition}
    Let $X=\{X_t\}_{t\in \mathcal{T}}$ and $Y=\{Y_t\}_{t\in \mathcal{T}}$ be stochastic processes.
    \begin{itemize}[nl]
        \item We say that $X$ and $Y$ are called \defn{indistinguishable} if almost all their sample paths agree; in other words,
            \begin{equation*}
                \pr(\omega\in\Omega:X_t(\omega)=Y_t(\omega)\text{ for all }t\in \mathcal{T})=1.
            \end{equation*}
        \item We say that $Y$ is a \defn{modification} of $X$ if for each $t\in \mathcal{T}$ we have $\pr(\omega\in\Omega:X_t(\omega)=Y_t(\omega))=1$.
        \item Finally, $X$ and $Y$ are said to have the \defn{same distribution} if all the finite dimensional distributions agree.
            In other words, if for all $n\in\N$ and $t_1<\cdots<t_n$, with $t_i\in \mathcal{T}$, we have $(X_{t_1},\ldots,X_{t_n})\overset{d}{=}(Y_{t_1},\ldots,Y_{t_n})$.
    \end{itemize}
\end{definition}
\begin{example}
    Let $X$ be a continuous stochastic process and $N$ a Poisson point process on $[0,\infty)$.
    Then define
    \begin{equation*}
        Y_t :=
        \begin{cases}
            X_t &: t\notin N\\
            X_t+1 &:t\in N
        \end{cases}
    \end{equation*}
    Thus $\pr(X_t=Y_t)=1$ for all $t$, so $X$ is a modification of $Y$.
    However, $\pr(X_t=Y_t,t\geq 0)=0$, so that $X$ and $Y$ are not indistinguishable.
\end{example}
A filtration formalizes the idea of ``information acquired over time''.
\begin{definition}
    Let $(\Omega,\mathcal{F},\pr)$ be a probability space.
    A \defn{filtration} is a non-decreasing family $\{\mathcal{F}_t\}_{t\in \mathcal{T}}$ of sub-$\sigma$-algebras of $\mathcal{F}$ so that $\mathcal{F}_s\subseteq\mathcal{F}_t\subseteq\mathcal{F}$ for any $s<t$ in $\mathcal{T}$.
    We write $F_\infty=\sigma(\bigcup_{t\in \mathcal{T}}\mathcal{F}_t)$.
\end{definition}
Let $\{X_t\}_{t\in \mathcal{T}}$ be a stochastic process.
\begin{definition}
    The \defn{filtration generated by $\{X_t\}_{t\in \mathcal{T}}$} is $\{\sigma(\{X_s:s\leq t\})\}_{t\in \mathcal{T}}$.
    In other words $\mathcal{F}_t$ is the smallest $\sigma$-algebra which makes $X_s$ measurable for all $s\leq t$ in $\mathcal{T}$.
    A stochastic process $\{X_t\}_{t\in \mathcal{T}}$ is called \defn{adapted} to a filtration $\{\mathcal{F}_t\}_{t\in \mathcal{T}}$ if $X_t$ is $\mathcal{F}_t$-measurable for every $t\in \mathcal{T}$.
\end{definition}
Clearly, the filtration generated by $\{X_t\}_{t\in \mathcal{T}}$ is the smallest filtration which makes $(X_t)_{t\in \mathcal{T}}$ adapted.
\begin{definition}
    A filtration $\{\mathcal{F}_t\}_{t\in \mathcal{T}}$ is said to satisfy the ``usual condition'' if
    \begin{enumerate}[nl,r]
        \item it is right-continuous: $\lim_{s\to t^+}:=\bigcap_{s>t}\mathcal{F}_s=\mathcal{F}_t$, and
        \item $\mathcal{F}_0$ contains all the $\pr-$null events in $\mathcal{F}$.
    \end{enumerate}
\end{definition}
\section{Martingale Theory}
\subsection{Stopping Times}
Consider a filtered space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in \mathcal{T}},\pr)$, i.e. a probability space equipped with a filtration.
\begin{definition}
    A random time $N:\Omega\to \mathcal{T}$ is called a \defn{stopping time} if $N^{-1}([0,t])\in\mathcal{F}_t$ for each $t\in \mathcal{T}$.
\end{definition}
The philosophy here is that we know that a stopping time happens when it happens.
\begin{example}
    \begin{enumerate}[nl,r]
        \item Constants are trivial stopping times.
        \item Last hit a constant before some fixed bound is not a stopping time
    \end{enumerate}
\end{example}
\begin{proposition}
    If $T,S$ are stopping times, $\min\{T,S\}$, $\max\{T,S\}$, and $T+S$ are stopping times.
\end{proposition}
\begin{proof}
    \begin{itemize}[nl]
        \item We have $\min\{T,S\}^{-1}([0,t])=T^{-1}([0,t])\cap S^{-1}([0,t])\in\mathcal{F}_t$.
        \item Similarly, we have $\max\{T,S\}^{-1}([0,t])=T^{-1}([0,t])\cup S^{-1}([0,t])\in\mathcal{F}_t$.
        \item For $T+S$, we have that
            \begin{align*}
                (T+S)^{-1}((t,\infty))&=\bigl(T^{-1}(\{0\})\cap S^{-1}((t,\infty))\bigr)\cup S^{-1}((t,\infty))\\
                                      &\agspace \cup\bigl(T^{-1}((0,t])\cap (T+S)^{-1}((t,\infty))\bigr)
            \end{align*}
            where $T^{-1}(\{0\})\cap S^{-1}((t,\infty))\in\mathcal{F}_t$ and $S^{-1}((t,\infty))\in\mathcal{F}_t$.
            To finish the proof, since $T$ has a countable dense subset $Q$, we have
            \begin{equation*}
                T^{-1}((0,t])\cap (T+S)^{-1}((t,\infty))=\bigcup_{r\in Q\cap(0,t)}S^{-1}((r,t])\cap T^{-1}((t-r,\infty))
            \end{equation*}
            where the expressions in the union are certainly $\mathcal{F}_t$-measurable.
    \end{itemize}
\end{proof}
\begin{definition}
    The \defn{$\sigma-$algebra generated by a stopping time $T$} is the set of all events $A$ for which $A\cap T^{-1}([0,t])\in\mathcal{F}_t$ for every $t\in \mathcal{T}$.
\end{definition}
Intuitively, this is the information you collect until the stopping time.
Note that this $\sigma$-algebra is not the same as the $\sigma$-algebra generated by the random variable $T$.
It is left as an exercise to the reader to verify that the above collection is indeed a $\sigma$-algebra.

We write $X_{T\wedge t}$ is a random variable evaluated at time $T\wedge t$ (or $T$); in other words, $(X_{T\wedge t})(\omega)=X_{T\wedge t}(\omega)$.
Then $\{X_{T\wedge t}\}_{t\in\mathcal{T}}$, or $X^T$, is a stochastic process stopped at time $t$.
\subsection{Doob's Upcrossing Inequality}
\begin{definition}
    Consider a filtered probability space $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in\mathcal{T}},\pr)$.
    A $\{\mathcal{F}_t\}_{t\in \mathcal{T}}-$adapted proces $\{X_t\}_{t\in \mathcal{T}}$ is said to be a \defn{submartingale} if
    \begin{enumerate}[nl,r]
        \item for all $t\in \mathcal{T}$, $X_t\in L^1(\Omega,\mathcal{F},\ps)$, and
        \item for all $s<t$ where $s,t\in \mathcal{T}$,
            \begin{equation*}
                \E(X_t | \mathcal{F}_s)\geq X_s
            \end{equation*}
            almost surely.
    \end{enumerate}
    and is said to be a \defn{supermartingale} if condition (ii) above is replaced with
    \begin{enumerate}[nl,r]
        \item[(ii')] for all $s<t$ where $s,t\in \mathcal{T}$,
            \begin{equation*}
                \E(X_t | \mathcal{F}_s)\leq X_s
            \end{equation*}
            almost surely.
    \end{enumerate}
    Then $\{X_t\}_{t\in \mathcal{T}}$ is a \defn{martingale} if it is both a submartingale and supermartingale.
\end{definition}
If $X$ is a submartingale and $0\leq s<t$ are fixed times, then $\E(X_0)\leq \E(X_s)\leq \E(X_t)$.
Similar statements hold in the (super)martingale cases as well.
One of the goals of martingale theory is to extend these results with respect to stopping times, rather than with respect to fixed times.

\begin{definition}
    Let $X=\{X_n\}_{n\in\Z^+}$ be a discrete time process and fix levels $a<b$ with $a,b\in\R$.
    Then the \defn{number of upcrossings} of $[a,b]$ by $X$ before time $N$ with respect to the event $\omega\in\Omega$, denoted by $U_N^X([a,b],\omega)$, is the maximal $k\in\Z^+$ such that there exists times $0\leq s_1<t_1<s_2<t_2<\cdots<s_k<t_k\leq N$ such that for all $i$, $X_{s_i}(\omega)\leq a$ and $X_{t_i}(\omega)\geq b$.
\end{definition}
Note that $U_N^X([a,b]):\Omega\to\Z^+$ given by $\omega\mapsto U_N^X([a,b],\omega)$ is a random variable.
\begin{definition}
    A process $C=\{C_n\}_{n\in\Z^+}$ is called \defn{previsible} if $C_n$ is $\mathcal{F}_{n-1}-$measurable for all $n\geq 1$.
    Suppose in addition that $\{X_n\}_{n\in\Z^+}$ is a discrete time process.
    Then the \defn{martingale transform} of $X$ by $C$, denoted by $C\cdot X$, is defined by
    \begin{equation*}
        (C\cdot X)_n=
        \begin{cases}
            \sum_{k=1}^n C_k(X_k-X_{k-1}) &: n>0\\
            0 &: n=0
        \end{cases}
    \end{equation*}
\end{definition}
\begin{lemma}\label{l:previs}
    Suppose $C$ is a bounded or $L^2$ previsible process.
    Then
    \begin{enumerate}[nl,r]
        \item Let $C$ be non-negative and $X$ a supermartingale.
            Then $C\cdot X$ is a supermartingale which is null at 0.
        \item Let $X$ be a martingale.
            Then $C\cdot X$ is a martingale which is null at 0.
    \end{enumerate}
\end{lemma}
\begin{proof}
    We first treat the case where $C$ is bounded.
    \begin{enumerate}[nl,r]
        \item We have by properties of conditional expectation and non-negativity of $C$
            \begin{align*}
                \E[(C\cdot X)_n-(C\cdot X)_{n-1}|\mathcal{F}_{n-1}] &= \E[C_n(X_n-X_{n-1})|\mathcal{F}_{n-1}]\\
                                                                    &= C_n\cdot \E[X_n-X_{n-1}|\mathcal{F}_{n-1}]\\
                                                                    &\leq 0
            \end{align*}
            where the second line follows since $C$ is previsible.
            Integrability follows immediately since $C$ is bounded and $X_n$ is $L^1$ for each $n\in\Z^+$.
        \item Consider $C+k$ where $k$ is a constant and $k\geq|C_n(w)|$ for all $n$ and $w$.
        \item Similar, but the integrability is now guaranteed by Hölder's inequality.
    \end{enumerate}
    % When $C$ is $L^2$, by Hölder's inequality, we have integrability of $C\cdot X$ given by
    % \begin{equation*}
    % \end{equation*}
\end{proof}
We now have the following result:
\begin{proposition}[Doob's Upcrossing Inequality]
    \begin{enumerate}[nl,r]
        \item Let $X$ be a supermartingale.
            Then
            \begin{equation*}
                (b-a)\cdot\E[U_N^X([a,b])]\leq\E[(X_N-a)^-].
            \end{equation*}
        \item Let $X$ be a submartingale.
            Then
            \begin{equation*}
                (b-a)\cdot\E[U_N^X([a,b])]\leq\E[(X_N-a)^+].
            \end{equation*}
    \end{enumerate}
\end{proposition}
\begin{proof}
    We prove (i); the proof of (ii) is analgous.
    Define a process $\{C_n\}_{n\in\Z^+}$ by
    \begin{align*}
        C_0&:= 0\\
        C_1&:=\chi_{X_0^{-1}((-\infty,a))}\\
        C_n&:=\chi_{C_{n-1}^{-1}(\{1\})}\cdot\chi_{X_{n-1}^{-1}((-\infty,b])}+\chi_{C_{n-1}^{-1}(\{0\})}\cdot\chi_{X_{n-1}^{-1}((-\infty,a))}.
    \end{align*}
    Certainly $C$ is non-negative and bounded; previsibility follows since $C_n$ is a characteristic function depending only on preimages of $X_k$ for $k<n$.
    Thus set $Y=C\cdot X$, i.e. $Y_n=\sum_{k=1}^n C_k\cdot(X_k-X_{k-1})$ almost surely; by \cref{l:previs}, $Y$ is a supermartingale.

    Since each finished upcrossing increases the value of $Y$ by at least $b-a$, we have for any $\omega\in\Omega$
    \begin{equation*}
        Y_N(\omega)\geq(b-a)U_N^X([a,b],\omega)-(X_n-a)^-(\omega)
    \end{equation*}
    where $(X_N-a)^-(\omega)$ is the upper bound for the ``loss'' due to the last unfinished upcrossing.
    Since $\E(Y_1)\leq 0$, we have $\E(Y_N)\leq\E(Y_1)\leq 0$.
    Thus
    \begin{equation*}
        (b-a)\cdot\E[U_N^X([a,b])]\leq\E[(X_n-a)^-]
    \end{equation*}
    as required.
\end{proof}
\subsection{Optimal Sampling Theorems}
\begin{theorem}
    Suppose $\{X_t\}_{t\in\mathcal{T}}$ is uniformly $L^2$.
    Then $X_t$ converge in $L^2$ to a limit $X_\infty$.
\end{theorem}
\begin{proof}
    Note that we have the result for both discrete and continuous time martingales.
    By discretization, it is clear that it suffices to rove the result for discrete case.

    We have the following orthogonality between the increments of a martingale $\{X_n\}_{n=0}^\infty$: if $n_1<n_2\leq n_2<n_4$, then
    \begin{equation*}
        \E[(X_{n_2}-X_{n_1})(X_{n_4}-X_{n_3})]=0.
    \end{equation*}
    This result follows by conditioning on $\mathcal{F}_{n_3}$ and applying the law of total expectation.

    Now the proof proceeds.
    Set $Y_n:=X_n-X_{n-1}$, so
    \begin{equation*}
        \norm{X_n}_2^2=\E(X_n^2)=\sum_{i=1}^n\norm{Y_i}_2^2
    \end{equation*}
    and $\sum_{i=0}^n\norm{Y_n}_2^2\leq B$ for all $n$, so that $\sum_{i=0}^\infty\norm{Y_n}_2^2\leq B$.
    Thus $\{X_n\}_{n=0}^\infty$ is Cauchy in $L^2(\Omega,\mathcal{F},\pr)$.
\end{proof}
\begin{theorem}[Optimal Sampling for Bounded Stopping Times in Discrete Times]
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ supermartingale and $S,T$ be $\{\mathcal{F}_n\}-$stopping times such that $0\leq S\leq T\leq N$ for some constant $N<\infty$.
    Then $X_T$ is integrable and $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely.
\end{theorem}
\begin{proof}
    Notice that
    \begin{equation*}
        |X_T|\leq|X_0|+|X_1|+\cdots+|X_N|
    \end{equation*}
    so that $\E(|X_T|)<\infty$.
    To prove that $\E(X_t|\mathcal{F}_s)\leq X_s$ a.s., it suffices to prove that $\E(X_T;A):=\int_A X_Td\ps\leq\int_A X_s d\ps=:\E(X_s;A)$ for all $A\in\mathcal{F}_s$.
    Assuming this, then
    \begin{equation*}
        \E(\E(X_T|\mathcal{F}_s)-X_s;A)\leq 0
    \end{equation*}
    for all $A\in\mathcal{F}_s$, so we may take $A=A_0:=\{\E(X_T|\mathcal{F}_s)-X_s>0\}$, so that $\pr(A_0)=0$.

    Let's prove the required statement.
    First note that
    \begin{equation*}
        \sum_{n=1}^N \chi_{\{S<n\leq T\}}(X_n-X_{n-1})=X_T-X_S
    \end{equation*}
    and taking expectation over $A$ on both sides
    \begin{align*}
        \E(X_T-X_S;A) &= \sum_{n=1}^N\E(\chi_{\{S<n\leq T\}}(X_n-X_{n-1});A)\\
                      &= \sum_{n=1}^N\E(X_n-X_{n-1};A\cap\{S<n\leq T\})
    \end{align*}
    But $A\cap\{S<n\leq T\}=A\cap\{S\leq n-1\}\cap\{n-1<T\}\in\mathcal{F}_{n-1}$.
    Thus
    \begin{equation*}
        \E(X_n-X_{n-1}; A\cap\{S<n\leq T\})=\E(\E(X_n-X_{n-1}|\mathcal{F}_{n-1});A\cap\{S<n\leq T\})\leq 0
    \end{equation*}
    so the required statement holds.
\end{proof}
\begin{definition}
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ a supermartingale.
    We say that $\{X_n\}_{n=0}^\infty$ is \defn{closed} by a random variable $X_\infty$ if $X_\infty$ is $\mathcal{F}_\infty-$measurable and $X_n\geq\E(X_\infty|\mathcal{F}_n)$ almost surely for all $n=0,1,\ldots$.
\end{definition}
Similar statements hold with $X_n\leq\E(X_\infty|\mathcal{F}_n)$ for a submartingale, or equality with a martingale.
\begin{proposition}
    Suppose that $\{X_n\}_{n=0}^\infty$ is a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ a non-negative supermartingale, and $X_\infty=0$.
    If $S,T:\Omega\to\overline{\Z^+}$ are $\{\mathcal{F}_n\}-$stopping times, $S\leq T$, then
    \begin{enumerate}[nl]
        \item $\E(X_T)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{enumerate}[nl]
        \item Note that $X_T\leq\liminf_{n\to\infty}X_{T\wedge n}$ where $T\wedge n$ and $0$ are two bounded stopping times.
            Thus $\E(X_{T\wedge n})\leq\E(X_0)$ for all $n=0,1,\ldots$.
            Thus by Fatou's lemma
            \begin{align*}
                \E(X_T)&\leq\E(\liminf_{n\to\infty} X_{T\wedge n})\leq\liminf_{n\to\infty}\E(X_{T\wedge n})\\
                       &\leq\E(X_0)<\infty.
            \end{align*}
        \item Let $A\in\mathcal{F}_s$.
            For $n=0,1,\ldots,$,
            \begin{align*}
                \E(X_T;A\cap\{T\leq n\}) &= \E(X_{T\wedge n};A\cap\{T\leq n\})\\
                                         &\leq \E(X_{T\wedge n};A\cap\{S\leq n\})\\
                                         &\leq \E(X_{S\wedge n}; A\cap\{S\leq n\})
            \end{align*}
            Note that $S\wedge n$ and $T\wedge n$ are two bounded stopping times with $S\wedge n\leq T\wedge n$.
            Also, $A\cap\{S\leq n\}\in\mathcal{F}_{S\wedge n}$.
            Then apply the optimal sampling theorem for bounded stopping times.
            By the monotone convergence theorem,
            \begin{equation*}
                \lim_{n\to\infty}\E(X_T;A\cap\{T\leq n\})
            \end{equation*}
            and similarly for $S$.
            Thus
            \begin{equation*}
                \E(X_T;A\cap\{T<\infty\})\leq\E(X_S:A\cap\{S<\infty\})
            \end{equation*}
            so that
            \begin{equation*}
                \E(X_T;A\cap\{T=\infty\})=\E(X_S:A\cap\{S=\infty\})=0
            \end{equation*}
            and $\E(X_T;A)\leq\E(X_S;A)$.
            Since this holds for all $A\in\mathcal{F}_s$, $\E(X_T:\mathcal{F}_s)\leq X_s$.
    \end{enumerate}
\end{proof}
\begin{lemma}
    Let $\{M_n\}_{n=0}^\infty$ be a martingale closed by $M_\infty$.
    Let $T:\Omega\to\overline{\Z^+}$ be a stopping time.
    Then $M_T=\E(M_\infty|\mathcal{F}_T)$.
\end{lemma}
\begin{proof}
    First assume $M_\infty\geq 0$.
    For any $A\in\mathcal{F}_T$,
    \begin{align*}
        E(M_T;A)&=\sum_{n=0}^\infty\E(M_n;A\cap\{T=n\})\\
                &=\sum_{n=0}^\infty\E(M_\infty,A\cap\{T=n\})\\
                &= \E(M_\infty;A).
    \end{align*}
    For the general case, decompose $M_\infty$ into positive and negative parts.
\end{proof}
\begin{theorem}[Optimal Sampling for Closed Supermartingales in Discrete Time]
    Let $\{X_n\}_{n=0}^\infty$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_n\}_{n=0}^\infty,\pr)$ supermartingale closed by $X_\infty$.
    Let $S,T:\Omega\to\overline{\Z^+}$ be two $\{\mathcal{F}_n\}_{n=0}^\infty$ stopping times with $S\leq T$.
    Then
    \begin{enumerate}[nl]
        \item $\E(|X_T|)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[nl]
        \item Define $M_n=\E(X_\infty|\mathcal{F}_n)$ and $A_n=X_n-M_n$ for $n=0,1,\ldots,\infty$.
            Since $\{X_n\}_{n=0}^\infty$ is a supermartingale closed by $X_\infty$, $A_n\geq 0$, $A_\infty=0$, and for $m\leq n$,
            \begin{align*}
                \E(A_n|\mathcal{F}_m)&=\E(X_n-\E(X_\infty|\mathcal{F}_n)|\mathcal{F}_m)\\
                                     &\leq X_m-\E(X_\infty|\mathcal{F}_m)\\
                                     &= A_m
            \end{align*}
            Thus $\{A_n\}_{n=0}^\infty$ is a non-negative supermartingale with $A_\infty=0$.

            One can prove that $\{M_n\}_{n=0}^\infty$ is a uniformly integrable martingale, i.e. $\lim_{c\to\infty}\sup_{n\in\N}\E(|X_n|;|X_n|>c)=0$.

            By the previous proposition, $\E(A_T)<\infty$.
            On the other hand, $\E(|M_T|)<\infty$ by definition of the $\{M_n\}$.
            Thus $\E(|X_T|)<\infty$.
        \item Apply the previous lemma so $\E(M_T|\mathcal{F}_s)=\E(\E(M_\infty|\mathcal{F}_T)\mathcal{F}_S)=\E(M_\infty|\mathcal{F}_s)=M_s$ by the optimal sampling theorem for closed martingales.
            Meanwhile, as $\{A_n\}_{n=0}^\infty$ is a non-negative supermartingale with $A_\infty=0$, $\E(A_T|\mathcal{F}_s)\leq A_s$ almost surely.
            Thus $\E(X_T|\mathcal{F}_s)\leq X_s$ almost surely.
    \end{enumerate}
\end{proof}
To prepare for the optimal sampling theorem for closed supermartingales in continuous time, we introduce one more discrete time notion:
\begin{definition}
    Denote by $\Z^-=\{z\in\Z:z\leq 0\}$.
    A \defn{negatively indexed supermartingale} $\{X_n\}_{n\in\Z^-}$ and $\{\mathcal{F}_n\}_{n\in\Z^-}$ where $\mathcal{F}_m\subseteq\mathcal{F}_n$ for $m\leq n$.
    Then $X_n\in L^1(\Omega,\mathcal{F},\pr)$, $\{X_n\}$ is adapted and $\E(X_n|\mathcal{F}_m)\leq X_m$.
\end{definition}
\begin{theorem}
    Let $\{X_n\}_{n\in\Z^-}$ be a negatively indexed supermartingale such that $\sup_{n\in\Z^-}\E(X_n)<\infty$.
    Then $\{X_n\}_{n\in\Z^-}$ is uniformly $L^1$.
\end{theorem}
\begin{proof}
    Fix $a=\sup_{n\in\Z^-}\E(X_n)$.
    For any $\epsilon>0$, get $N(\epsilon)\in\Z^-$ such that $\E(X_{N(\epsilon)})>a-\epsilon$ for $N\leq N(\epsilon)$.
    Since $X$ is a supermartingale, $0\leq\E(X_n)-\E(X_{N(\epsilon)})\leq\epsilon$ for all $n\leq N(\epsilon)$.
    For $c>0$, note that
    \begin{equation*}
        |x|\chi_{|X|>c}=-x\chi_{X<-c}-X\chi_{X\leq x}+x.
    \end{equation*}
    Since $\{X_n\}$ is a supermartingale, for $n\leq N(\epsilon)$, $\E(X_{N(\epsilon)}|\mathcal{F}_n)\leq X_n$ and $\{X_n<-c\}\in\mathcal{F}_n$.
    Thus $\E(X_n;X_n<-c)\geq\E(X_{N(\epsilon)};X_n<-c)$.
    Similarly, $\E(X_n;X_n\leq c)\geq\E(X_{N(\epsilon)};X_n\leq c)$.
    Moreover, $\E(X_n)\leq\E(X_{N(\epsilon)})+\epsilon$, so $\E(|X_n|;X_n\leq c)\leq\E(X_{N(\epsilon)};X_n\leq c)+\epsilon$ for all $N\leq N(\epsilon)$.
    For this $N(\epsilon)$, there exists $\delta(\epsilon)>0$ such that $\E(X_{N(\epsilon)};A)<\epsilon$ for all $A\in\mathcal{F}$ and $\pr(A)<\delta(\epsilon)$.

    Since $X$ is a supermartingale, $\{-2X_n^-\}_{n\in\Z^-}$ is also a supermartingale.
    To see this, define $f(x)=2\min\{x,0\}$, so $f$ is increasing and concave.
    But then for $M\leq n$, by Jensen's inequality,
    \begin{align*}
        \E(f(X_n)|\mathcal{F}_m) &\leq f(\E(X_n|\mathcal{F}_m))\leq f(X_m)
    \end{align*}
    and $-2X_n^-=f(X_n)$ is thus a supermartingale, so $\E(-2X_n^-)\geq\E(-2X_0^-)$.
    We also have $\E(X_n)\leq a$ where $a$ is defined as above.
    Thus since $|X_n|=X_n+2X_n^-$, we have
    \begin{equation*}
        \E(|X_n|)\leq a-\E(-2X_0^-)=a+2\E(X_0^-)<\infty.
    \end{equation*}

    Now, choose $c$ so that $c\cdot\delta(\epsilon)>a+2\E(X_0^-)\geq\E(|X_n|)$.
    Then by Markov's ineuality, $\pr(|X_n|>c)\leq\delta(\epsilon)$.
    Thus $\E(|X_{N(\epsilon)}|;|X_n|>c)<\epsilon$ for $n\in\Z^-$ so that $\E(|X_m|;|X_n|>c)<2\epsilon$ for all $n<N(\epsilon)$.
    But there are only finitely many terms with $n>N(\epsilon)$, so we are done.
\end{proof}
\begin{theorem}[Optimal Sampling Theorem for Closed Supermartingales]
    Let $\{X_t\}_{t\in\R^+}$ be an $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in\R^+},\pr)$ supermartingale which is right continuous and closed.
    Let $S,T:\Omega\to\overline{\R^+}$ be two stopping times with $S\leq T$.
    Then
    \begin{enumerate}[nl,r]
        \item $\E(|X_T|)<\infty$
        \item $\E(X_T|\mathcal{F}_s)\leq X_s$ a.s.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Define random times $T_n:=2^{-n}(\lfloor 2^nT\rfloor+1)$.
    Then the $T_n$ are decreasing stopping times with $T=\lim T_n$ pointwise from above.
    For each $n$, $\{X_{m2^{-n}}\}_{n\in\Z^+}$ is a discrete-time closed supermartngale.
    Thus $\E(X_{T_n-1}|\mathcal{F}_{T_n})\leq X_{T_n}$ a.s. (optimal sampling theorem for closed supermartingale in discrete time).
    Set $Y_n=X_{T_n-1}$, so $\{Y_n\}_{n=-1,-2,\ldots}$ is a negatively indexed supermartingale and $\E(Y_n)=\E(X_{T-n-1})\leq\E(X_0)<\infty$.
    Thus by the previous result, $\{X_{T_n}\}_{n\in\Z^+}$ is uniformly $L^1$.
    Since $T_n\to T$ from above and $X_{T_n}\to X_T$ a.s., uniform integrability gives that $X_{T_n}\to X_T$ in $L^1$.
    Thus $\E(|X_T|)=\lim_{n\to\infty}\E(|X_{T_n}|)<\infty$.
    Similarly, define $S_n=2^{-n}(\lfloor 2^nS\rfloor+1)$.
    Both $S_n$ and $T_n$ can be regarded as discrete-time stopping times.
    Thus by the discrete optimal sampling theorem, $\E(X_{T_n}|\mathcal{F}_{S_n})\leq X_{S_n}$ a.s.

    Now for $A\in\mathcal{F}_S\subseteq\mathcal{F}_{S_n}$, $\E(X_{T_n};A)\leq\E(X_{S_n};A)$ so
    \begin{equation*}
        |\E(X_{T_n};A)-\E(X_T;A)|\;eq\E(|X_{T_n}-X_T|)\fto{n\to\infty} c
    \end{equation*}
    Also, $|\E(X_{S_n};A)-\E(X_S;A)|\to 0$.
    Therefore, as $n$ goes to infinity,
    \begin{equation*}
        \E(X_T;A)\leq\E(X_S;A)
    \end{equation*}
    for all $A\in\mathcal{F}_S$ so $\E(X_T|\mathcal{F}_S)\leq X_s$.
\end{proof}
\begin{corollary}
    Let $\{X_t\}_{t\in\R^+}$ be a $(\Omega,\mathcal{F},\{\mathcal{F}_t\}_{t\in\R^+}$ martingale which is right continuous and closed.
    If $S,T:\Omega\to\overline{\R^+}$ are two stopping times with $S\leq T$, then $X_T$ is integrable and $|E(X_T|\mathcal{F}_S)=X_S$.
\end{corollary}
\begin{corollary}
    Same results hold for bounded $S,T$ and general right continuous martingale.
\end{corollary}
\begin{corollary}
    A right continuous adapted process $X$ is a martingale if and only if for every bounded stopping time $T$, $X_T$ is $L^1$ and $\E(X_T)=\E(X_0)$.
\end{corollary}
\begin{proof}
    The forward direction is contained in the above theorem.
    Conversely, for any $s<t$ and $A\in\mathcal{F}_s$, define $T=t\chi_{A^c}+s\chi_{A}$ which is a stopping time (exercise).
    Then $\E(X_0)=\E(X_T)=\E(X_t;A^c)+\E(X_s;A)$.
    On the other hand, $\E(X_0)=\E(X_t)=\E(X_t;A^c)+\E(X_t;A)$ so $\E(X_s;A)=\E(X_t;A)$.
    But $A\in\mathcal{F}_s$ is arbitrary, so $\E(X_t|\mathcal{F}_s)=\E(X_s)$.
\end{proof}
\begin{corollary}
    Suppose $\{X_t\}_{t\in\mathcal{T}}$ is a martingale and $T$ is a stopping time.
    Then the stopped process $X^T$ where $X^T_t=X_{t\wedge T}$ for each $t\in\mathcal{T}$ is also a martingale.
\end{corollary}
\begin{proof}
    The process $X^T$ is clearly adapted.
    For any bounded stopping time $S$, $S\min T$ is also a bounded stopping time.
    Then
    \begin{equation*}
        \E(X_s^T)=\E(X_{S\wedge T})=\E(X_0)=\E(X_0^T)
    \end{equation*}
    so $X^T$ is a martingale by the previous corollary.
\end{proof}
\subsection{Martingale Convergence}
\begin{theorem}
    Let $\{X_t\}_{t\in\mathcal{T}}$ be a supermartingale.
    If $\sup_t\E(X_t^-)<\infty$, then $\lim_{t\to\infty}X_t$ exists almost surely.
\end{theorem}
\begin{proof}
    Suppose $\lim_{t\to\infty}X_t$ does not exist a.s.
    Then there exist levels $a$ and $b$ such that $X$ upcrosses $[a,b]$ infinitely many times with positive probability.
    In other words, $\pr(\lim_{N\to\infty}U_N([a,b])=\infty)>0$, so $\E[U_N([a,b])]\to\infty$ as $N\to\infty$.
    However,
    \begin{equation*}
        \E(U_N[a,b])\leq\frac{1}{b-a}\sup_t\E[(X_t-a)^-]\leq\frac{1}{b-1}\sup_t(\E(X_t^-)+|a|)<\infty.
    \end{equation*}
\end{proof}
\begin{corollary}
    A positive supermartingale converges a.s.
\end{corollary}
\begin{theorem}
    Let $\{X_t\}_{t\in\mathcal{T}}$ be a martingale.
    The following conditions are equivalent:
    \begin{enumerate}[nl,r]
        \item $\lim_{t\to\infty}X_t$ converges in $L^1$
        \item There is a random variable $X_\infty$ in $L^1$ such that $X_t=\E(X_\infty|\mathcal{F}_t)$, i.e. $X$ is closed by $X_\infty$
        \item $X$ is uniformly $L^1$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    First note that the upcrossing number $U_n^X([a,b],\omega)$ can be defined analgously for a continuous time process.
    Moreover, the upcrossing inequality sill holds in the continuous time case (right continuity + discretization).

    \imp{ii}{iii}
    Discussed earlier (find proof)

    \imp{iii}{i}
    Exercise: U.I. implies $\sup_t\E(X_t^-)<\infty$.
    Thus by the supermartingale convergence theorem, $\lim_{t\to\infty}X_t$ exists a.s.
    Then U.I. converges to $X_\infty$ in $L^1$.

    \imp{i}{ii}
    $X+t-\E(X_{t+h}|\mathcal{F}_t)$ and take limit as $H\to\infty$ (??)
\end{proof}
\begin{corollary}
    Let $\{X_t\}_{t\in\mathcal{T}}$ be a U.I. martingale.
    If $S,T:\Omega\to\overline{T}$ are two stopping times with $S\leq T$.
    Then
    \begin{enumerate}[nl,r]
        \item $\E(|X_T|)<\infty$
        \item $\E(X_T|\mathcal{F}_s)=X_s$
    \end{enumerate}
\end{corollary}
\begin{corollary}
    Let $\{X_t\}_{t\in\mathcal{T}}$ be a martingale and let $T$ be a stopping time such that $|X_{t\wedge T}|\leq 0$ for all $t\in\mathcal{T}$.
    Then $\E(X_T)=\E(X_0)$ where $X_T=\lim_{t\to\infty}X_{t\wedge T}$.
\end{corollary}
\begin{proof}
    The stopped process $X^T$ is a bounded martingale and hence U.I.
    Thus $X_T=\lim_{t\to\infty}X_{t\wedge T}$ exists a.s. and $X_t^T\to X_T$ in $L^1$.
    Thus $\E(X^T_t)=\E(X_0)$ implies $E(X_T)=\E(X_0)$.
\end{proof}
\section{Brownian Motion}
\begin{definition}
    An adapted process $B=\{B_t\}_{t\geq 0}$ is called a (standard, one-dimensional) \defn{Brownian motion} if
    \begin{enumerate}[nl]
        \item $B_0=0$ a.s.
        \item $B$ has continuous sample paths
        \item $B_t-B_s\perp\mathcal{F}_s$ (define independent), $B_t-B_s\sim \mathcal{N}(0,t-s)$.
    \end{enumerate}
    Equivalently:
    \begin{enumerate}[nl]
        \item[1'] $\{B_t\}_{t\geq 0}$ is a Gaussian process
        \item[2'] $B$ has continuous paths
        \item[3'] $\E(B_t)=0$, $\E(B_sB_t)=s\wedge t$ for $s,t\geq 0$.
    \end{enumerate}
\end{definition}
\imp{3}{1'}
immediate

\imp{1+3}{3'}
$\E(B_t)=0$ for $t\geq 0$.
Assume $s\leq t$.
Then $\E(B_sB_t)=\E(B_s^2)+\E(B_s(B_t-B_s))$, where $\E(B_s(B_t-B_s))=0$ by condition on $\mathcal{F}_s$, and $\E(B_s^2)\sim N(0,s)$.

\imp{3'}{1}
$\E(B_0^2)=0$ so $B_0=0$ a.s.

\imp{1'+3'}{3}
(3') determines the variance-covariance structure for a Gaussian process, which coincides with the distribution given by (3).
\begin{theorem}[Levy's Characterization]
    Let $X$ be a $\mathcal{F}_t$-adapted continuousprocess vanishing at 0.
    Then $X$ is an $\mathcal{F}_t$-Brownian motion if and only if both $X$ and $X_t^2-t$ are martinales.
\end{theorem}
\begin{proof}
    ``only if'': Clearly $X$ is a martingale.
    Then $X_t^2-t$ is a martingale since
    \begin{align*}
        \E(X_t^2-t|\mathcal{F}_s)&=\E[((X_t-X_s)+X_s)^2|\mathcal{F}_s]-t\\
                                 &=\E[(X_t-X_s)^2|\mathcal{F}_s]+X_s^2-t\\
                                 &=t-s+X_s^2=X+s^2-s.
    \end{align*}
\end{proof}
\begin{proposition}
    Brownian motion is a strong Markov process.
    Let $T$ be a stopping time; then $B_T^*:=\{B_{T+t}-B_t\}_{t\geq 0}$ is a Brownian motion, independent of $\mathcal{F}_T$.
\end{proposition}
\begin{proof}
    Discretization and discussing all the possible values of $T$.
\end{proof}
Scaling property of BM: $\{B_{at}\}_{t\geq 0}=\{a^{1/2}B_t\}_{t\geq 0}$ in distribution.
\begin{theorem}
    Let $B=\{B_t\}_{t\geq 0}$ be a Brownian motion.
    Then the process $X_0=0$ and $X_t=tB(1/t)$ is also a Brownian motion.
\end{theorem}
TODO: check.
\begin{corollary}
    $\pr(\inf\{t>0:B(t)=0\}=0)=1$, i.e. 0 is almost surely an accumulation point of zeros of $\{B_t\}_{t\geq 0}$.
\end{corollary}
\begin{proposition}[Reflection Principle]
    Let $a>0$, $\tau_a:=\inf\{t:B(t)=a\}$, and $M_t:=\sup_{s\in[0,t]}B_s$.
    Then $\pr(M_t\geq a)=\pr(\tau_a\leq t)=2\pr(B_t\geq a)$.
\end{proposition}
\begin{proof}
    We have
    \begin{align*}
        \pr(\tau_a\leq t) &= \pr(\tau_a\leq t,B_t\geq a)+\pr(\tau_a\leq t, B_t<a)\\
                          &= \pr(\tau_a\leq t,B_t-B_{\tau_a}\geq 0)\\
                          &= \E(\pr(B^*_{t-\tau_a}\geq 0|\tau_a);\tau_a\leq t)
    \end{align*}
    Similarly,
    \begin{equation*}
        \pr(\tau_a\leq t,B_t<a) = \E(\pr(B^*_{t-\tau_a}<0|\tau_a);\tau_a\leq t)
    \end{equation*}
    and by symmetry
    \begin{equation*}
        \pr(B^*_{t-\tau_a}\geq 0|\tau_a)=\pr(B^*_{t-\tau_a}<0|\tau_a)=\frac{1}{2}
    \end{equation*}
    so that
    \begin{equation*}
        \pr(\tau_a\leq t,B_t\geq a)=\pr(\tau_a\leq t,B_t<a)
    \end{equation*}
\end{proof}
\begin{theorem}
    Almost surely the path of a Brownian motion is of unbounded variation on any compact interval.
\end{theorem}
\begin{proof}
    Let $\{\pi_n\}_{n=1}^\infty$ be an increasing sequence of partitions of $[a,b]$ with part size converging uniformly to 0.
    Consider
    \begin{equation*}
        \sum_{t_i\in\pi_n}(B_{t_{i+1}}-B_{t_i})^2-(b-a)=\sum_{t_i\in\pi_n}\underbrace{[(B_{t_{i+1}}-B_{t_i})^2-(t_{i+1}-t_i)]}_{\text{ind. rvs mean 0}}.
    \end{equation*}
    Moreover, note that
    \begin{equation*}
        \frac{(B_{t_{i+1}})^2}{t_{i+1}-t_i}\sim Z^2
    \end{equation*}
    where $Z\sim N(0,1)$.
    Thus
    \begin{align*}
        \E\left[\left(\sum_{t_i\in\pi_n}[(B_{t_{i+1}}-B_{t_i})^2-(t_{i+1}-t_i)]\right)^2\right] &= \sum_{t_i\in\pi_n}\E\left[\left((B_{t_{i+1}}-B_{t_i})^2-(t_{i+1}-t_i)^2\right)\right]\\
                                                                                                &= \sum_{t_i\in\pi_n}(t_{i+1}-t_i)^2\E((Z^2-1)^2)\\
                                                                                                &\leq\E((Z^2-1)^2)\sup_{t_i\in\pi_n}|t_{i+1}-t_i|\cdot(b-a)
    \end{align*}
    which converges to $0$ as $n\to\infty$.
    Thus $\sum_{t_i\in\pi_n}(B_{t_{i+1}}-B_{t_i})^2\to b-a$ in $L^2$.
    Thus there exists a subsequence $(\pi_{n_k})_{k=1}^\infty$ such that $\sum_{t_i\in\pi_n}(B_{t_i+1}-B_{t_i})^2\to b-a$ almost surely.
    On the other hand,
    \begin{align*}
        \sum_{t_i\in\pi_{n_k}}(B_{t_i+1}-B_{t_i})^2 &\leq\sup_{t_i\in\pi_{n_k}}|B_{t_{i+1}}-B_{t_i}|\cdot\sum_{t_i\in\pi_{m_n}}|B_{t_{i+1}}-B_{t_i}|\\
                                                    &\leq\sup_{t_i\in\pi_{n_k}}|B_{t_{i+1}}-B_{t_i}|\cdot TV_{[a,b]}(B)
    \end{align*}
    Since each path $B$ is uniformly continuous on $[a,b]$, since the part size of $\pi_{n_k}\to 0$, $\sup_{t_i\in\pi_{n_k}}|B_{t_{i+1}}-B_{t_i}|\to 0$.
    Then as $n\to\infty$ on both sides,
    \begin{equation*}
        b-a\leq 0\cdot TV_{[a,b]}(B)
    \end{equation*}
    so that $TV_{[a,b]}(B)=\infty$.
\end{proof}
\section{Stochastic Integration}
Our goal is to define $\int fdB=\int f(t,w)dB_t(w)$.
Since $B$ does not have bounded variation, we cannot define this integral as a pathwise Lebesgue-Stieltjes integral.
Let $I=[a,b]$ and define $V=V_I$ be the class of processes $f:[0,\infty)\times\Omega\to\R$ such that
\begin{enumerate}[nl,r]
    \item $f$ is $\mathcal{B}\times\mathcal{F}$-measurable
    \item $f$ is $\mathcal{F}_t$-adapted.
    \item $\E(\int_a^Bf^2(t,\omega)dt)<\infty$, i.e. $f\in L^2([a,b]\times\Omega)$.
\end{enumerate}
A process $\phi\in V$ is called \defn{elementary} if $\phi$ has the form
\begin{equation*}
    \phi=\sum_j A_j\chi_{[t_j,t_{j+1})}(t)
\end{equation*}
where each $A_j$ is a $\mathcal{F}_{t_j}$-measurable random variable for all $j$, and $\{t_j\}$ forms a partition of $I$.
We then define
\begin{equation*}
    \int_I\phi dB=\sum_J A_j(B_{t_{j+1}}-B_{t_j}).
\end{equation*}
Next, we extend this definition to any process in $V$ by approaching the process using elementtary process.
\begin{lemma}
    Let $g\in V$ be bounded and continuous.
    Then there exists $\{g_n\}_{n=1}^\infty$, $g_n\in V$ elementary, such that $g_n\to g$ in $L^2$.
\end{lemma}
\begin{proof}
    Fix a partition $\pi_n=\{t_j\}$, we define
    \begin{equation*}
        \phi_n=\sum g(t_j)\idc{[t_k,t_{j+1})}.
    \end{equation*}
    Since $g$ is $\mathcal{B}\otimes\mathcal{F}$-measurable and $\mathcal{F}_t$-adapted, so is $\phi_n$.
    Moreover, since $g$ is bounded, $\phi_n$ is also bounded, so $\phi_n\in V$ and $\phi_n$ is an elementary function.
    But $\int_a^b(g-\phi_n)^2dt\to 0$ pointwise, so that $g\to\phi_n$ in $L^2$ by the dominated convergence theorem.
\end{proof}
\begin{lemma}
    Let $h\in V$ be a bounded process.
    Then there exists $\{g_n\}\subseteq V$, $g_n$ bounded and continuous, such that $g_n\to h$ in $L^2$.
\end{lemma}
\begin{proof}
    We define mollifiers
    \begin{align*}
        \rho(t)&=
        \begin{cases}
            c\cdot\exp\left(\frac{-1}{1-t^2}\right) &:|t|<1\\
            0 &:\text{otherwise}
        \end{cases}\\
        \rho_\epsilon(t)&=\frac{1}{\epsilon}\rho(\epsilon^{-1}t)
    \end{align*}
    Define $h(t)=0$ for $t<a$ and let
    \begin{align*}
        g_\epsilon(t)&=g_\epsilon * h(t)\\
                     &= \frac{1}{\epsilon}\int_{t-2\epsilon}^t\rho\left(\frac{t-s-\epsilon}{\epsilon}\right)h(s)\d{s}\\
                     &= \frac{1}{\epsilon}\int_{-\epsilon}^{\epsilon}\rho\left(\frac{z}{\epsilon}\right)h(t-z-\epsilon)\d{z}\\
                     &= \int_{-\epsilon}^{\epsilon}\rho_\epsilon(z)h(t-z-\epsilon)\d{z}
    \end{align*}
    We first see that $g_\epsilon$ is adapted.
    By Cauchy-Schwarz, we have
    \begin{align*}
        \int_a^b g_\epsilon^2(t)\d{t} &\leq \int_a^b\left(\int_{-\epsilon}^\epsilon\rho_\epsilon(z)\d{z}\cdot\int_{-\epsilon}^\epsilon\rho_\epsilon(z)h^2(t-z-\epsilon)\d{z}\right)\d{t}\\
                                      &\leq \int_{-\epsilon}^\epsilon\rho_\epsilon(z)\left(\int_{a-1}^b h^2(t)\d{t}\right)\d{z}\\
                                      &= \int_a^b h^2(t)\d{t}
    \end{align*}
    and taking expectations, we have that $g_\epsilon\in L^2$.

    It is clear that $g_\epsilon$ is bounded and continuous for any $\epsilon$.

    Next, we show convegence.
    Fix some $\omega\in\Omega$.
    Get $(u_n)_{n=1}^\infty$ such that $u_n(t)=0$ and $u_n(t)\to h_n(t,\omega)$ in $L^2$.
    Since $u_n$ is uniformly continuous, $\rho_\epsilon*u_n\to u_n$ uniformly on $[a,b]$.
    Thus
    \begin{align*}
        \norm{\rho_\epsilon*h(\cdot,\omega)-h(\cdot,\omega)}_2 &\leq\norm{\rho_\epsilon*h(\cdot,\omega)-\rho_\epsilon*u_n(\cdot)}_2\\
                                                               &\agspace+\norm{\rho_\epsilon*u_n-u_n}_2+\norm{u_n-h(\cdot,\omega)}_2\\
                                                               &\leq 2\norm{u_n-h(\cdot,\omega)}_2+\norm{\rho_\epsilon*u_n-u_n}_2
    \end{align*}
    which both converge to $0$ as $n\to\infty$.
    Since $g_\epsilon=\rho_\epsilon*h$ and $h$ are uniformly bounded (by the bound of $h$), dominated convergence applies and we have $\E(\int_a^b(h-g_n)^2\d{t})\to 0$ as $n\to\infty$.
\end{proof}
\begin{lemma}
    Let $f\in V$.
    Then there exists a sequence $\{h_n\}\subset V$ such that $h_n$ is bounded for all $n$ and $\E(\int_a^b|f-h_n|^2\d{t})\to 0$ as $n\to\infty$.
\end{lemma}
\begin{proof}
    Take
    \begin{equation*}
        h_n =
        \begin{cases}
            -n &: f<-n\\
            f &: |f|\leq n\\
             &: f>n
        \end{cases}
    \end{equation*}
    and the result follows by the dominated convergence theorem.
\end{proof}
Combing the proceding lemmas, we have:

For any process $f\in V$, there exists elementary processes $\phi_n\in V$ such that $\E(\int_a^b|f-\phi_n|^2\d{t})\to 0$ as $n\to\infty$.
We want to define $\int_a^b f\d{B}$ as the limit of $\int_a^b\phi_n\d{B}$.
To this end, we still need the existence and uniqueness of the limit.
\begin{lemma}[Itô's Isometry for Elementary Processes]
    Let $\phi$ be elementary.
    Then $\E[(\int_a^b\phi(t)\d{b_t})^2]=\E(\int_a^b\phi^2(t)\d{t})$.
\end{lemma}
\begin{proof}
    Define $\Delta B_j=B_{j+1}-B_j$.
    Then
    \begin{equation*}
        \E(A_iA_j\Delta B_i\Delta B_j) =
        \begin{cases}
            \E(A_j^2)\cdot(t_{j+1}-t_j) &: i=j\\
            0 &: i\neq j
        \end{cases}
    \end{equation*}
    Thus
    \begin{align*}
        \E[(\int_a^b\phi\d{B})^2] &= \E[(\sum_j A_j\Delta B_j)^2] =\sum_{i,j}\E(A_iA_j\Delta B_i\Delta B_j)\\
                                  &= \sum_j\E(A_j^2)(t_{j+1}-t_j)\\
                                  &= \E\int_a^b\phi(t)^2\d{t}
    \end{align*}
\end{proof}
By Lemma 4, since $\phi_n$ is a Cauchy sequence in $L^2(\pr\times\lambda|_{[a,b]})$, $\int_a^b\phi_n(t)\d{B_t}$ is a Cauchy sequence in $L^2(\pr)$.
Hence the limit exists.
Moreover, if $\E(\int_a^b|f-\phi_n|^2\d{t})\to 0$ and $\E(\int_a^b|f-\phi_n'|^2\d{t})\to 0$, then $\E[(\int_a^b\phi_n(t)\d{B_t})-\int_a^b\phi_n'(t)\d{B_t})^2]\to 0$.
The limit is unique, i.e. does not depend on the choice of $\phi_n$.
Thus we can define, for general $f\in V$,
\begin{equation*}
    \int_a^b f_t\d{B_t} := \lim_{n\to\infty}\int_a^b\phi_n(t)\d{B_t}
\end{equation*}
where $\{\phi_n\}$ is a sequence is an elementary process satisfying
\begin{equation*}
    \E(\int_a^b|f-\phi_n|^2\d{t})\to 0
\end{equation*}
in $L^2$.
\begin{corollary}[Itô's Isometry]
    Let $f\in V$, then
    \begin{equation*}
        \E[(\int_a^b f\d{B})^2]=\E(\int_a^b f^2\d{t})
    \end{equation*}
\end{corollary}
\begin{proof}
    This works for elementary processes; pass to the limit and use dominated convergence.
\end{proof}
\subsection{Properties of the Stochastic Integral}
The stochastic integral is linear.:
\begin{equation*}
    \int_a^b(cf+g)\d{B_t} = c\int_a^b f\d{B_t}+\int_a^b g\d{B_t}
\end{equation*}
In particular,
\begin{equation*}
    \int_a^b f\d{B_t}=\int_a^df\d{B_t}+\int_d^bf\d{B_t}
\end{equation*}
for any $a<d<b$.
\begin{proposition}
    Let $\{M_n\}_{n=0}^N$ be a submartingale.
    Then for any $\lambda>0$
    \begin{equation*}
        \lambda\pr(\sup_n M_n\geq\lambda)\leq\E(|M_N|\chi_{\{\sup_n M_n\geq\lambda\}})
    \end{equation*}
\end{proposition}
\begin{proof}
    Set $T:=\in\{n:M_n\geq\lambda\}$ if such an $n$ exists, and $N$ otherwise.
    Since $T$ is a bounded stopping time, so is $N$.
    By the optimal sampling theorem,
    \begin{align*}
        \E(M_N)&\geq\E(M_T)\\
               &= \E(M_T\idc{\sup_n M_n\geq\lambda})+\E(M_T\idc{\sup_n M_n<\lambda})\\
               &\geq \lambda\cdot\pr(\sup_n M_n\geq\lambda)+\E(M_N\idc{\sup_n M_n<\lambda})
    \end{align*}
    so that
    \begin{align*}
        \lambda\pr(\sup_n M_n\geq\lambda) &\leq\E(M_n\idc{\sup_n M_n\geq\lambda})\\
                                          &\leq\E(|M_N|\idc{\sup_n M_n\geq\lambda})
    \end{align*}
    as required.
\end{proof}
\begin{corollary}
    Let $M=\{M_n\}_{n=0}^N$ be a martingale or a positive supermartingale.
    Then for any $P\geq 1$ and $\lambda>0$,
    \begin{equation*}
        \lambda^=\pr(\sup_N|M_n|\geq\lambda)\leq\E(|M_N|^p)
    \end{equation*}
\end{corollary}
\begin{proof}
    By Jensen's inequality, $|M_n|^p$ is a submartingale.
    Apply the above proposition to $|M_n|^p$ and $\lambda^p$ so that
    \begin{align*}
        \lambda^p\pr(\sup_n|M_n|^p\geq\lambda^p)\leq\E(|M_N|^p\idc{\sup_n|M_n|^p\geq\lambda^p})\leq\E(|M_N|^p)
    \end{align*}
\end{proof}
\begin{theorem}[Doob's Martingale Inequality]
    Let $M_t$ be a continuous martingale.
    Then for $p\geq 1$, $T\geq 0$, and $\lambda>0$,
    \begin{equation*}
        \pr(\sup_{0\leq t\leq T}|M_t|\geq\lambda)\leq\frac{1}{\lambda^p}\E(|M_T|^p).
    \end{equation*}
\end{theorem}
\begin{proof}
    Let $D$ be a dense coutable subset of $[0,T]$ and $D_n$ an increasing sequence of finite subsets of $D$ such that $\bigcup_{n=1}^\infty D_n=D$.
    On each $D_n$, we can apply the discrete time result
    \begin{equation*}
        \lambda^p\pr(\sup_{t\in D_n}|M_t|\geq\lambda)\leq\E(|M_{D_n^*}|^p)
    \end{equation*}
    for $p\geq 1$, where $D_n^*=\max(D_n)$.
    Then as $n\to\infty$, for any $\epsilon>0$,
    \begin{align*}
        \pr(\sup_{t\in D}|M_t|\geq\lambda)&\leq\pr(\sup_{t\in D}|M_t|)>\lambda-\epsilon\\
                                          &=\lim_{n\to\infty}\pr(\sup_{t\in D_n}|M_t|>\lambda-\epsilon)\\
                                          &\leq\liminf_{n\to\infty}\pr(\sup_{t\in D_n}|M_t|\geq\lambda-\epsilon)\\
                                          &= \liminf_{n\to\infty}\frac{1}{(\lambda-\epsilon)^p}\E(|M_{D_n^*}|^p).
    \end{align*}
    Then as $\epsilon\to 0$,
    \begin{align*}
        \pr(\sup_{t\in D}|M_t|\geq\lambda) &\leq\lambda^{-p}\liminf_{n\to\infty}\E(|M_{D_n^*}|^p)\\
                                           &\leq\lambda^{-p}\E(|M_T|^p).
    \end{align*}
\end{proof}
\begin{theorem}
    Let $f\in V(0,T)$ for all $T\in\R^+$.
    Then $M_t=\int_0^tf_s\d{B_s}$ has a continuous version, which is a $\mathcal{F}_t$-martingale.
    Moreover,
    \begin{equation*}
        \pr\left(\sup_{0\leq t\leq T}|M_t|\geq\lambda\right)\leq\frac{1}{\lambda^2}\E(\int_0^Tf^2\d{s})
    \end{equation*}
    for $\lambda,T>0$.
\end{theorem}
\begin{proof}
    Let $\{\phi_n\}$ be a sequence of elementary processes such that $\E(\int_0^t(f-\phi_n)^2\d{t})\to 0$ as $n\to\infty$.
    Define processes
    \begin{equation*}
        I_n(t)=\int_0^t\phi_n(s)\d{B_s}.
    \end{equation*}
    Since $\phi_n=\sum_j A_j^{(n)}\idx{[t_j,t_{j+1})}(t)$, we have
    \begin{equation*}
        I_n(t)=\sum_{j\leq k-1}A_j^{(n)}(B_{t_{j+1}}-B_{t_j})+A_k^{(n)}(B_t-B_{t_k})
    \end{equation*}
    for $t\in[t_k,t_{k+1}]$.
    It is easy to check that $I_n$ is a continuous martingale for any $n$.
    Thus $I_n-I_m$ is also a continuous martingale for any $m,n$.
    By Doob's martingale inequality and Itô's isometry
    \begin{align*}
        \pr(\sup_{0\leq t\leq T}|I_n(t)-I_m(t)|>\epsilon) &\leq\frac{1}{\epsilon^2}\E(|I_n(T)-I_m(T)|^2)\\
                                                          &= \frac{1}{\epsilon^2}\E(\int_0^T(\phi_n-\phi_m)^2\d{s})
    \end{align*}
    which converges to $0$ as $m,n\to\infty$.
    Thus there exists a subsequence $(n_k)_{k=1}^\infty$ such that
    \begin{equation*}
        \pr(\sup_{0\leq t\leq T}|I_{n_{k+1}}-I_{n_k}|>2^{-k})<2^{-k}.
    \end{equation*}
    Thus by Borel-Cantelli,
    \begin{equation*}
        \pr(\sup_{0\leq t\leq T}|I_{n_{k+1}}(t)-I_{n_k}(t)|>2^{-k}\text{ infinitely often})=0.
    \end{equation*}
    Therefore, almost surely, there exists $k_1=k_1(\omega)$ such that
    \begin{equation*}
        \sup_{0\leq t\leq T}|I_{n_{k+1}}(t)-I_{n_k}(t)|\leq 2^{-k}
    \end{equation*}
    for all $k\geq k_1$.
    Thus $I_{n_k}(t)$ converges uniformly for $t\in[0,T]$ to some $I(t)$.
    Then $I(t)$ is continuous.
    On the other hand, $I_{n_k}(t)\to M(t)$ in $L^2(\pr)$ for any $t$, hence $M(t)=I(t)$ almost surely for any $t\in[0,T]$.
    Thus $I(t)$ is a continuous version of $M(t)$.

    Next, we always mean this continuous version when writing $M(t)$.
    $M(t)$ is certainly adapted (since it is the $L^2$ limit of $I_n(t)$).
    Moreover, for $t\geq 0$, since $I_n(t)\to M(t)$ in $L^2$, we also have
    \begin{equation*}
        \E(I_n(t)|\mathcal{G})\to \E(M(t)|\mathcal{G})
    \end{equation*}
    for any sub-$\sigma$-algebra $\mathcal{G}$ of $\mathcal{F}$.
    Thus
    \begin{align*}
        \E(M(t)|\mathcal{F}_s)&= \lim_{n\to\infty}\E(I_n(t)|\mathcal{F}_s)=\lim_{n\to\infty}I_n(s)\\
                              &= M(s)
    \end{align*}
    so that $M(t)$ is a $(\mathcal{F}_t)_{t\in \mathcal{I}}$-martingale.
    Then by Doob's martingale inequality,
    \begin{align*}
        \pr(\sup_{0\leq t\leq T}|M_t|\geq\lambda)\leq\frac{1}{\lambda^2}\E(M_T^2)=\frac{1}{\lambda^2}\E(\int_0^Tf^2(s)\d{s})
    \end{align*}
\end{proof}
\subsection{Extensions of Itô's Integral}
Stochastic integral with stopping time
\begin{lemma}
    Let $f\in V(0,T)$ and $T$ a stopping time.
    Set $m(t)=\int_0^Tf_s\d{B_s}$.
    Then $M(T)=\int_0^Tf(s)\d{B_s}=\int_0^Tf(s)\idx{s<T}\d{B_s}$.
\end{lemma}
\end{document}
